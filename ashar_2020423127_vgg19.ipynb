{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ca543687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bfef498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n",
    "start_time = time.time()\n",
    "batch_size = 40\n",
    "learning_rate = 0.001\n",
    "num_of_epochs = 15\n",
    "root_dir = 'C/Users/MYDocuments/Deep Learning Project/VGG_shahg1'\n",
    "default_directory = 'C/Users/MYDocuments/Deep Learning Project/VGG_shahg1'\n",
    "val_losses = []\n",
    "train_losses = []\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "70abfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Data Augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),               # Random Position Crop\n",
    "    transforms.RandomHorizontalFlip(),                  # right and left flip\n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),                               \n",
    "    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010) )  # RGB Normalize Standard Deviation\n",
    "])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e13e503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    " # Automatic downloading datasets and data augmentation\n",
    "train_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                 train=True,\n",
    "                                 transform=transform_train,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=root_dir,\n",
    "                                train=False,\n",
    "                                transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n",
    "                                           num_workers=4)           # CPU loader number\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n",
    "                                          num_workers=4)            # CPU loader number\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1ab1d064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#loading Pretrainined VGG\\nmodel= models.vgg16(pretrained=True)\\nmodel.classifier[6] = nn.Linear(4096, 10)\\n\\n'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up the processing device and initializing the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def initialize_model( num_classes, use_pretrained=True):\n",
    "\n",
    "    model_ft = models.vgg19_bn(pretrained=use_pretrained)\n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "    input_size = 224\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model, input_size = initialize_model(10, use_pretrained=True)\n",
    "'''\n",
    "#loading Pretrainined VGG\n",
    "model= models.vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "412e2f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE ONLY CPU!\n"
     ]
    }
   ],
   "source": [
    "# Defining optimizer\n",
    "model = model.to(device)\n",
    "#optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10) #Changed the optimizer to Adagrad \n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"USE ONLY CPU!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "785c172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "        \n",
    "        TRAIN_LOSS.append(train_loss)\n",
    "        TRAIN_ACC.append((100. * correct / total))\n",
    "\n",
    "            \n",
    "    dict = {'train loss' : TRAIN_LOSS, 'train accuracy' : (100. * correct / total)}\n",
    "    df = pd.DataFrame(dict)\n",
    "    df.to_csv('train_loss_and_acc_vgg.csv')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1f22d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the test function\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        writer.add_scalar('Loss/val', test_loss, epoch)\n",
    "        \n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        \n",
    "        TEST_LOSS.append(test_loss)\n",
    "        TEST_ACC.append((100. * correct / total))\n",
    "        \n",
    "    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n",
    "          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "\n",
    "            \n",
    "    dict = {'test loss' : TEST_LOSS, 'train accuracy' : (100. * correct / total)}\n",
    "    df = pd.DataFrame(dict)\n",
    "    df.to_csv('test_loss_and_acc_vgg.csv')\n",
    "\n",
    "\n",
    "def save_checkpoint(directory, state, filename='latest.tar.gz'):\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    torch.save(state, model_filename)\n",
    "    print(\"=> saving checkpoint\")\n",
    "\n",
    "def load_checkpoint(directory, filename='latest.tar.gz'):\n",
    "\n",
    "    model_filename = os.path.join(directory, filename)\n",
    "    if os.path.exists(model_filename):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        state = torch.load(model_filename)\n",
    "        return state\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "Epoch: 2 | Batch_idx: 0 |  Loss: (0.0587) | Acc: (100.00%) (40/40)\n",
      "Epoch: 2 | Batch_idx: 10 |  Loss: (0.0555) | Acc: (98.41%) (433/440)\n",
      "Epoch: 2 | Batch_idx: 20 |  Loss: (0.0591) | Acc: (98.33%) (826/840)\n",
      "Epoch: 2 | Batch_idx: 30 |  Loss: (0.0519) | Acc: (98.55%) (1222/1240)\n",
      "Epoch: 2 | Batch_idx: 40 |  Loss: (0.0585) | Acc: (98.29%) (1612/1640)\n",
      "Epoch: 2 | Batch_idx: 50 |  Loss: (0.0552) | Acc: (98.38%) (2007/2040)\n",
      "Epoch: 2 | Batch_idx: 60 |  Loss: (0.0538) | Acc: (98.36%) (2400/2440)\n",
      "Epoch: 2 | Batch_idx: 70 |  Loss: (0.0531) | Acc: (98.45%) (2796/2840)\n",
      "Epoch: 2 | Batch_idx: 80 |  Loss: (0.0557) | Acc: (98.36%) (3187/3240)\n",
      "Epoch: 2 | Batch_idx: 90 |  Loss: (0.0568) | Acc: (98.30%) (3578/3640)\n",
      "Epoch: 2 | Batch_idx: 100 |  Loss: (0.0581) | Acc: (98.27%) (3970/4040)\n",
      "Epoch: 2 | Batch_idx: 110 |  Loss: (0.0599) | Acc: (98.20%) (4360/4440)\n",
      "Epoch: 2 | Batch_idx: 120 |  Loss: (0.0582) | Acc: (98.24%) (4755/4840)\n",
      "Epoch: 2 | Batch_idx: 130 |  Loss: (0.0573) | Acc: (98.26%) (5149/5240)\n",
      "Epoch: 2 | Batch_idx: 140 |  Loss: (0.0578) | Acc: (98.24%) (5541/5640)\n",
      "Epoch: 2 | Batch_idx: 150 |  Loss: (0.0574) | Acc: (98.23%) (5933/6040)\n",
      "Epoch: 2 | Batch_idx: 160 |  Loss: (0.0573) | Acc: (98.21%) (6325/6440)\n",
      "Epoch: 2 | Batch_idx: 170 |  Loss: (0.0577) | Acc: (98.19%) (6716/6840)\n",
      "Epoch: 2 | Batch_idx: 180 |  Loss: (0.0565) | Acc: (98.23%) (7112/7240)\n",
      "Epoch: 2 | Batch_idx: 190 |  Loss: (0.0580) | Acc: (98.18%) (7501/7640)\n",
      "Epoch: 2 | Batch_idx: 200 |  Loss: (0.0579) | Acc: (98.16%) (7892/8040)\n",
      "Epoch: 2 | Batch_idx: 210 |  Loss: (0.0562) | Acc: (98.23%) (8291/8440)\n",
      "Epoch: 2 | Batch_idx: 220 |  Loss: (0.0569) | Acc: (98.19%) (8680/8840)\n",
      "Epoch: 2 | Batch_idx: 230 |  Loss: (0.0577) | Acc: (98.14%) (9068/9240)\n",
      "Epoch: 2 | Batch_idx: 240 |  Loss: (0.0568) | Acc: (98.17%) (9464/9640)\n",
      "Epoch: 2 | Batch_idx: 250 |  Loss: (0.0558) | Acc: (98.20%) (9859/10040)\n",
      "Epoch: 2 | Batch_idx: 260 |  Loss: (0.0553) | Acc: (98.20%) (10252/10440)\n",
      "Epoch: 2 | Batch_idx: 270 |  Loss: (0.0556) | Acc: (98.21%) (10646/10840)\n",
      "Epoch: 2 | Batch_idx: 280 |  Loss: (0.0551) | Acc: (98.22%) (11040/11240)\n",
      "Epoch: 2 | Batch_idx: 290 |  Loss: (0.0563) | Acc: (98.21%) (11432/11640)\n",
      "Epoch: 2 | Batch_idx: 300 |  Loss: (0.0562) | Acc: (98.25%) (11829/12040)\n",
      "Epoch: 2 | Batch_idx: 310 |  Loss: (0.0562) | Acc: (98.24%) (12221/12440)\n",
      "Epoch: 2 | Batch_idx: 320 |  Loss: (0.0585) | Acc: (98.18%) (12606/12840)\n",
      "Epoch: 2 | Batch_idx: 330 |  Loss: (0.0589) | Acc: (98.14%) (12994/13240)\n",
      "Epoch: 2 | Batch_idx: 340 |  Loss: (0.0581) | Acc: (98.17%) (13390/13640)\n",
      "Epoch: 2 | Batch_idx: 350 |  Loss: (0.0577) | Acc: (98.17%) (13783/14040)\n",
      "Epoch: 2 | Batch_idx: 360 |  Loss: (0.0579) | Acc: (98.17%) (14176/14440)\n",
      "Epoch: 2 | Batch_idx: 370 |  Loss: (0.0581) | Acc: (98.18%) (14570/14840)\n",
      "Epoch: 2 | Batch_idx: 380 |  Loss: (0.0580) | Acc: (98.18%) (14962/15240)\n",
      "Epoch: 2 | Batch_idx: 390 |  Loss: (0.0582) | Acc: (98.15%) (15351/15640)\n",
      "Epoch: 2 | Batch_idx: 400 |  Loss: (0.0584) | Acc: (98.15%) (15743/16040)\n",
      "Epoch: 2 | Batch_idx: 410 |  Loss: (0.0585) | Acc: (98.14%) (16135/16440)\n",
      "Epoch: 2 | Batch_idx: 420 |  Loss: (0.0586) | Acc: (98.15%) (16529/16840)\n",
      "Epoch: 2 | Batch_idx: 430 |  Loss: (0.0585) | Acc: (98.14%) (16919/17240)\n",
      "Epoch: 2 | Batch_idx: 440 |  Loss: (0.0594) | Acc: (98.11%) (17306/17640)\n",
      "Epoch: 2 | Batch_idx: 450 |  Loss: (0.0601) | Acc: (98.08%) (17693/18040)\n",
      "Epoch: 2 | Batch_idx: 460 |  Loss: (0.0597) | Acc: (98.09%) (18088/18440)\n",
      "Epoch: 2 | Batch_idx: 470 |  Loss: (0.0603) | Acc: (98.06%) (18474/18840)\n",
      "Epoch: 2 | Batch_idx: 480 |  Loss: (0.0605) | Acc: (98.05%) (18865/19240)\n",
      "Epoch: 2 | Batch_idx: 490 |  Loss: (0.0605) | Acc: (98.03%) (19254/19640)\n",
      "Epoch: 2 | Batch_idx: 500 |  Loss: (0.0602) | Acc: (98.03%) (19646/20040)\n",
      "Epoch: 2 | Batch_idx: 510 |  Loss: (0.0611) | Acc: (98.01%) (20033/20440)\n",
      "Epoch: 2 | Batch_idx: 520 |  Loss: (0.0609) | Acc: (98.01%) (20426/20840)\n",
      "Epoch: 2 | Batch_idx: 530 |  Loss: (0.0615) | Acc: (97.98%) (20812/21240)\n",
      "Epoch: 2 | Batch_idx: 540 |  Loss: (0.0616) | Acc: (97.96%) (21199/21640)\n",
      "Epoch: 2 | Batch_idx: 550 |  Loss: (0.0622) | Acc: (97.94%) (21586/22040)\n",
      "Epoch: 2 | Batch_idx: 560 |  Loss: (0.0626) | Acc: (97.92%) (21973/22440)\n",
      "Epoch: 2 | Batch_idx: 570 |  Loss: (0.0625) | Acc: (97.92%) (22365/22840)\n",
      "Epoch: 2 | Batch_idx: 580 |  Loss: (0.0623) | Acc: (97.93%) (22758/23240)\n",
      "Epoch: 2 | Batch_idx: 590 |  Loss: (0.0623) | Acc: (97.94%) (23152/23640)\n",
      "Epoch: 2 | Batch_idx: 600 |  Loss: (0.0624) | Acc: (97.92%) (23540/24040)\n",
      "Epoch: 2 | Batch_idx: 610 |  Loss: (0.0621) | Acc: (97.93%) (23933/24440)\n",
      "Epoch: 2 | Batch_idx: 620 |  Loss: (0.0619) | Acc: (97.93%) (24327/24840)\n",
      "Epoch: 2 | Batch_idx: 630 |  Loss: (0.0618) | Acc: (97.94%) (24720/25240)\n",
      "Epoch: 2 | Batch_idx: 640 |  Loss: (0.0616) | Acc: (97.94%) (25113/25640)\n",
      "Epoch: 2 | Batch_idx: 650 |  Loss: (0.0612) | Acc: (97.96%) (25509/26040)\n",
      "Epoch: 2 | Batch_idx: 660 |  Loss: (0.0613) | Acc: (97.95%) (25898/26440)\n",
      "Epoch: 2 | Batch_idx: 670 |  Loss: (0.0614) | Acc: (97.95%) (26290/26840)\n",
      "Epoch: 2 | Batch_idx: 680 |  Loss: (0.0619) | Acc: (97.94%) (26680/27240)\n",
      "Epoch: 2 | Batch_idx: 690 |  Loss: (0.0623) | Acc: (97.92%) (27064/27640)\n",
      "Epoch: 2 | Batch_idx: 700 |  Loss: (0.0620) | Acc: (97.92%) (27458/28040)\n",
      "Epoch: 2 | Batch_idx: 710 |  Loss: (0.0620) | Acc: (97.93%) (27852/28440)\n",
      "Epoch: 2 | Batch_idx: 720 |  Loss: (0.0616) | Acc: (97.95%) (28248/28840)\n",
      "Epoch: 2 | Batch_idx: 730 |  Loss: (0.0621) | Acc: (97.94%) (28639/29240)\n",
      "Epoch: 2 | Batch_idx: 740 |  Loss: (0.0621) | Acc: (97.96%) (29034/29640)\n",
      "Epoch: 2 | Batch_idx: 750 |  Loss: (0.0630) | Acc: (97.92%) (29415/30040)\n",
      "Epoch: 2 | Batch_idx: 760 |  Loss: (0.0630) | Acc: (97.92%) (29808/30440)\n",
      "Epoch: 2 | Batch_idx: 770 |  Loss: (0.0628) | Acc: (97.93%) (30202/30840)\n",
      "Epoch: 2 | Batch_idx: 780 |  Loss: (0.0628) | Acc: (97.93%) (30593/31240)\n",
      "Epoch: 2 | Batch_idx: 790 |  Loss: (0.0629) | Acc: (97.91%) (30980/31640)\n",
      "Epoch: 2 | Batch_idx: 800 |  Loss: (0.0628) | Acc: (97.92%) (31373/32040)\n",
      "Epoch: 2 | Batch_idx: 810 |  Loss: (0.0630) | Acc: (97.91%) (31762/32440)\n",
      "Epoch: 2 | Batch_idx: 820 |  Loss: (0.0629) | Acc: (97.91%) (32155/32840)\n",
      "Epoch: 2 | Batch_idx: 830 |  Loss: (0.0626) | Acc: (97.92%) (32550/33240)\n",
      "Epoch: 2 | Batch_idx: 840 |  Loss: (0.0624) | Acc: (97.93%) (32942/33640)\n",
      "Epoch: 2 | Batch_idx: 850 |  Loss: (0.0628) | Acc: (97.91%) (33329/34040)\n",
      "Epoch: 2 | Batch_idx: 860 |  Loss: (0.0627) | Acc: (97.92%) (33723/34440)\n",
      "Epoch: 2 | Batch_idx: 870 |  Loss: (0.0629) | Acc: (97.91%) (34113/34840)\n",
      "Epoch: 2 | Batch_idx: 880 |  Loss: (0.0631) | Acc: (97.91%) (34505/35240)\n",
      "Epoch: 2 | Batch_idx: 890 |  Loss: (0.0631) | Acc: (97.91%) (34896/35640)\n",
      "Epoch: 2 | Batch_idx: 900 |  Loss: (0.0632) | Acc: (97.91%) (35288/36040)\n",
      "Epoch: 2 | Batch_idx: 910 |  Loss: (0.0633) | Acc: (97.92%) (35682/36440)\n",
      "Epoch: 2 | Batch_idx: 920 |  Loss: (0.0634) | Acc: (97.92%) (36074/36840)\n",
      "Epoch: 2 | Batch_idx: 930 |  Loss: (0.0631) | Acc: (97.92%) (36467/37240)\n",
      "Epoch: 2 | Batch_idx: 940 |  Loss: (0.0634) | Acc: (97.92%) (36858/37640)\n",
      "Epoch: 2 | Batch_idx: 950 |  Loss: (0.0635) | Acc: (97.92%) (37249/38040)\n",
      "Epoch: 2 | Batch_idx: 960 |  Loss: (0.0637) | Acc: (97.91%) (37635/38440)\n",
      "Epoch: 2 | Batch_idx: 970 |  Loss: (0.0635) | Acc: (97.92%) (38032/38840)\n",
      "Epoch: 2 | Batch_idx: 980 |  Loss: (0.0636) | Acc: (97.92%) (38422/39240)\n",
      "Epoch: 2 | Batch_idx: 990 |  Loss: (0.0637) | Acc: (97.91%) (38812/39640)\n",
      "Epoch: 2 | Batch_idx: 1000 |  Loss: (0.0638) | Acc: (97.90%) (39201/40040)\n",
      "Epoch: 2 | Batch_idx: 1010 |  Loss: (0.0641) | Acc: (97.90%) (39589/40440)\n",
      "Epoch: 2 | Batch_idx: 1020 |  Loss: (0.0644) | Acc: (97.89%) (39978/40840)\n",
      "Epoch: 2 | Batch_idx: 1030 |  Loss: (0.0645) | Acc: (97.88%) (40367/41240)\n",
      "Epoch: 2 | Batch_idx: 1040 |  Loss: (0.0646) | Acc: (97.88%) (40757/41640)\n",
      "Epoch: 2 | Batch_idx: 1050 |  Loss: (0.0648) | Acc: (97.87%) (41143/42040)\n",
      "Epoch: 2 | Batch_idx: 1060 |  Loss: (0.0650) | Acc: (97.86%) (41531/42440)\n",
      "Epoch: 2 | Batch_idx: 1070 |  Loss: (0.0649) | Acc: (97.86%) (41925/42840)\n",
      "Epoch: 2 | Batch_idx: 1080 |  Loss: (0.0648) | Acc: (97.87%) (42317/43240)\n",
      "Epoch: 2 | Batch_idx: 1090 |  Loss: (0.0649) | Acc: (97.87%) (42709/43640)\n",
      "Epoch: 2 | Batch_idx: 1100 |  Loss: (0.0648) | Acc: (97.87%) (43100/44040)\n",
      "Epoch: 2 | Batch_idx: 1110 |  Loss: (0.0649) | Acc: (97.87%) (43494/44440)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Batch_idx: 1120 |  Loss: (0.0649) | Acc: (97.87%) (43883/44840)\n",
      "Epoch: 2 | Batch_idx: 1130 |  Loss: (0.0649) | Acc: (97.86%) (44274/45240)\n",
      "Epoch: 2 | Batch_idx: 1140 |  Loss: (0.0652) | Acc: (97.86%) (44662/45640)\n",
      "Epoch: 2 | Batch_idx: 1150 |  Loss: (0.0652) | Acc: (97.86%) (45057/46040)\n",
      "Epoch: 2 | Batch_idx: 1160 |  Loss: (0.0651) | Acc: (97.86%) (45448/46440)\n",
      "Epoch: 2 | Batch_idx: 1170 |  Loss: (0.0649) | Acc: (97.87%) (45843/46840)\n",
      "Epoch: 2 | Batch_idx: 1180 |  Loss: (0.0650) | Acc: (97.87%) (46234/47240)\n",
      "Epoch: 2 | Batch_idx: 1190 |  Loss: (0.0649) | Acc: (97.88%) (46629/47640)\n",
      "Epoch: 2 | Batch_idx: 1200 |  Loss: (0.0646) | Acc: (97.88%) (47023/48040)\n",
      "Epoch: 2 | Batch_idx: 1210 |  Loss: (0.0647) | Acc: (97.88%) (47415/48440)\n",
      "Epoch: 2 | Batch_idx: 1220 |  Loss: (0.0648) | Acc: (97.88%) (47804/48840)\n",
      "Epoch: 2 | Batch_idx: 1230 |  Loss: (0.0648) | Acc: (97.88%) (48198/49240)\n",
      "Epoch: 2 | Batch_idx: 1240 |  Loss: (0.0648) | Acc: (97.89%) (48591/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1529) | Acc: (95.32%) (9532/10000)\n",
      "Epoch: 3 | Batch_idx: 0 |  Loss: (0.0487) | Acc: (97.50%) (39/40)\n",
      "Epoch: 3 | Batch_idx: 10 |  Loss: (0.0440) | Acc: (98.64%) (434/440)\n",
      "Epoch: 3 | Batch_idx: 20 |  Loss: (0.0382) | Acc: (98.81%) (830/840)\n",
      "Epoch: 3 | Batch_idx: 30 |  Loss: (0.0375) | Acc: (98.71%) (1224/1240)\n",
      "Epoch: 3 | Batch_idx: 40 |  Loss: (0.0375) | Acc: (98.60%) (1617/1640)\n",
      "Epoch: 3 | Batch_idx: 50 |  Loss: (0.0395) | Acc: (98.68%) (2013/2040)\n",
      "Epoch: 3 | Batch_idx: 60 |  Loss: (0.0451) | Acc: (98.61%) (2406/2440)\n",
      "Epoch: 3 | Batch_idx: 70 |  Loss: (0.0431) | Acc: (98.59%) (2800/2840)\n",
      "Epoch: 3 | Batch_idx: 80 |  Loss: (0.0410) | Acc: (98.70%) (3198/3240)\n",
      "Epoch: 3 | Batch_idx: 90 |  Loss: (0.0403) | Acc: (98.71%) (3593/3640)\n",
      "Epoch: 3 | Batch_idx: 100 |  Loss: (0.0384) | Acc: (98.76%) (3990/4040)\n",
      "Epoch: 3 | Batch_idx: 110 |  Loss: (0.0400) | Acc: (98.74%) (4384/4440)\n",
      "Epoch: 3 | Batch_idx: 120 |  Loss: (0.0388) | Acc: (98.78%) (4781/4840)\n",
      "Epoch: 3 | Batch_idx: 130 |  Loss: (0.0389) | Acc: (98.78%) (5176/5240)\n",
      "Epoch: 3 | Batch_idx: 140 |  Loss: (0.0391) | Acc: (98.81%) (5573/5640)\n",
      "Epoch: 3 | Batch_idx: 150 |  Loss: (0.0378) | Acc: (98.86%) (5971/6040)\n",
      "Epoch: 3 | Batch_idx: 160 |  Loss: (0.0388) | Acc: (98.79%) (6362/6440)\n",
      "Epoch: 3 | Batch_idx: 170 |  Loss: (0.0386) | Acc: (98.80%) (6758/6840)\n",
      "Epoch: 3 | Batch_idx: 180 |  Loss: (0.0380) | Acc: (98.81%) (7154/7240)\n",
      "Epoch: 3 | Batch_idx: 190 |  Loss: (0.0381) | Acc: (98.80%) (7548/7640)\n",
      "Epoch: 3 | Batch_idx: 200 |  Loss: (0.0376) | Acc: (98.82%) (7945/8040)\n",
      "Epoch: 3 | Batch_idx: 210 |  Loss: (0.0391) | Acc: (98.80%) (8339/8440)\n",
      "Epoch: 3 | Batch_idx: 220 |  Loss: (0.0394) | Acc: (98.81%) (8735/8840)\n",
      "Epoch: 3 | Batch_idx: 230 |  Loss: (0.0394) | Acc: (98.82%) (9131/9240)\n",
      "Epoch: 3 | Batch_idx: 240 |  Loss: (0.0392) | Acc: (98.82%) (9526/9640)\n",
      "Epoch: 3 | Batch_idx: 250 |  Loss: (0.0393) | Acc: (98.81%) (9921/10040)\n",
      "Epoch: 3 | Batch_idx: 260 |  Loss: (0.0383) | Acc: (98.85%) (10320/10440)\n",
      "Epoch: 3 | Batch_idx: 270 |  Loss: (0.0378) | Acc: (98.87%) (10718/10840)\n",
      "Epoch: 3 | Batch_idx: 280 |  Loss: (0.0385) | Acc: (98.87%) (11113/11240)\n",
      "Epoch: 3 | Batch_idx: 290 |  Loss: (0.0378) | Acc: (98.90%) (11512/11640)\n",
      "Epoch: 3 | Batch_idx: 300 |  Loss: (0.0381) | Acc: (98.86%) (11903/12040)\n",
      "Epoch: 3 | Batch_idx: 310 |  Loss: (0.0377) | Acc: (98.87%) (12299/12440)\n",
      "Epoch: 3 | Batch_idx: 320 |  Loss: (0.0373) | Acc: (98.87%) (12695/12840)\n",
      "Epoch: 3 | Batch_idx: 330 |  Loss: (0.0371) | Acc: (98.87%) (13090/13240)\n",
      "Epoch: 3 | Batch_idx: 340 |  Loss: (0.0369) | Acc: (98.88%) (13487/13640)\n",
      "Epoch: 3 | Batch_idx: 350 |  Loss: (0.0380) | Acc: (98.85%) (13879/14040)\n",
      "Epoch: 3 | Batch_idx: 360 |  Loss: (0.0374) | Acc: (98.87%) (14277/14440)\n",
      "Epoch: 3 | Batch_idx: 370 |  Loss: (0.0378) | Acc: (98.87%) (14672/14840)\n",
      "Epoch: 3 | Batch_idx: 380 |  Loss: (0.0376) | Acc: (98.86%) (15066/15240)\n",
      "Epoch: 3 | Batch_idx: 390 |  Loss: (0.0373) | Acc: (98.86%) (15461/15640)\n",
      "Epoch: 3 | Batch_idx: 400 |  Loss: (0.0373) | Acc: (98.86%) (15857/16040)\n",
      "Epoch: 3 | Batch_idx: 410 |  Loss: (0.0370) | Acc: (98.86%) (16253/16440)\n",
      "Epoch: 3 | Batch_idx: 420 |  Loss: (0.0372) | Acc: (98.86%) (16648/16840)\n",
      "Epoch: 3 | Batch_idx: 430 |  Loss: (0.0376) | Acc: (98.85%) (17041/17240)\n",
      "Epoch: 3 | Batch_idx: 440 |  Loss: (0.0378) | Acc: (98.85%) (17437/17640)\n",
      "Epoch: 3 | Batch_idx: 450 |  Loss: (0.0379) | Acc: (98.85%) (17832/18040)\n",
      "Epoch: 3 | Batch_idx: 460 |  Loss: (0.0375) | Acc: (98.86%) (18230/18440)\n",
      "Epoch: 3 | Batch_idx: 470 |  Loss: (0.0374) | Acc: (98.86%) (18625/18840)\n",
      "Epoch: 3 | Batch_idx: 480 |  Loss: (0.0377) | Acc: (98.87%) (19022/19240)\n",
      "Epoch: 3 | Batch_idx: 490 |  Loss: (0.0375) | Acc: (98.86%) (19417/19640)\n",
      "Epoch: 3 | Batch_idx: 500 |  Loss: (0.0374) | Acc: (98.87%) (19813/20040)\n",
      "Epoch: 3 | Batch_idx: 510 |  Loss: (0.0374) | Acc: (98.86%) (20208/20440)\n",
      "Epoch: 3 | Batch_idx: 520 |  Loss: (0.0374) | Acc: (98.86%) (20602/20840)\n",
      "Epoch: 3 | Batch_idx: 530 |  Loss: (0.0376) | Acc: (98.85%) (20996/21240)\n",
      "Epoch: 3 | Batch_idx: 540 |  Loss: (0.0379) | Acc: (98.84%) (21388/21640)\n",
      "Epoch: 3 | Batch_idx: 550 |  Loss: (0.0380) | Acc: (98.84%) (21785/22040)\n",
      "Epoch: 3 | Batch_idx: 560 |  Loss: (0.0383) | Acc: (98.83%) (22177/22440)\n",
      "Epoch: 3 | Batch_idx: 570 |  Loss: (0.0385) | Acc: (98.82%) (22571/22840)\n",
      "Epoch: 3 | Batch_idx: 580 |  Loss: (0.0384) | Acc: (98.83%) (22967/23240)\n",
      "Epoch: 3 | Batch_idx: 590 |  Loss: (0.0385) | Acc: (98.82%) (23361/23640)\n",
      "Epoch: 3 | Batch_idx: 600 |  Loss: (0.0388) | Acc: (98.81%) (23753/24040)\n",
      "Epoch: 3 | Batch_idx: 610 |  Loss: (0.0387) | Acc: (98.81%) (24148/24440)\n",
      "Epoch: 3 | Batch_idx: 620 |  Loss: (0.0385) | Acc: (98.82%) (24546/24840)\n",
      "Epoch: 3 | Batch_idx: 630 |  Loss: (0.0381) | Acc: (98.83%) (24945/25240)\n",
      "Epoch: 3 | Batch_idx: 640 |  Loss: (0.0381) | Acc: (98.83%) (25341/25640)\n",
      "Epoch: 3 | Batch_idx: 650 |  Loss: (0.0379) | Acc: (98.84%) (25739/26040)\n",
      "Epoch: 3 | Batch_idx: 660 |  Loss: (0.0378) | Acc: (98.85%) (26137/26440)\n",
      "Epoch: 3 | Batch_idx: 670 |  Loss: (0.0383) | Acc: (98.85%) (26531/26840)\n",
      "Epoch: 3 | Batch_idx: 680 |  Loss: (0.0386) | Acc: (98.83%) (26922/27240)\n",
      "Epoch: 3 | Batch_idx: 690 |  Loss: (0.0383) | Acc: (98.84%) (27320/27640)\n",
      "Epoch: 3 | Batch_idx: 700 |  Loss: (0.0382) | Acc: (98.84%) (27716/28040)\n",
      "Epoch: 3 | Batch_idx: 710 |  Loss: (0.0384) | Acc: (98.84%) (28111/28440)\n",
      "Epoch: 3 | Batch_idx: 720 |  Loss: (0.0384) | Acc: (98.83%) (28503/28840)\n",
      "Epoch: 3 | Batch_idx: 730 |  Loss: (0.0386) | Acc: (98.81%) (28893/29240)\n",
      "Epoch: 3 | Batch_idx: 740 |  Loss: (0.0388) | Acc: (98.80%) (29285/29640)\n",
      "Epoch: 3 | Batch_idx: 750 |  Loss: (0.0391) | Acc: (98.79%) (29676/30040)\n",
      "Epoch: 3 | Batch_idx: 760 |  Loss: (0.0391) | Acc: (98.78%) (30070/30440)\n",
      "Epoch: 3 | Batch_idx: 770 |  Loss: (0.0391) | Acc: (98.79%) (30467/30840)\n",
      "Epoch: 3 | Batch_idx: 780 |  Loss: (0.0392) | Acc: (98.78%) (30860/31240)\n",
      "Epoch: 3 | Batch_idx: 790 |  Loss: (0.0390) | Acc: (98.78%) (31255/31640)\n",
      "Epoch: 3 | Batch_idx: 800 |  Loss: (0.0391) | Acc: (98.78%) (31649/32040)\n",
      "Epoch: 3 | Batch_idx: 810 |  Loss: (0.0391) | Acc: (98.79%) (32046/32440)\n",
      "Epoch: 3 | Batch_idx: 820 |  Loss: (0.0395) | Acc: (98.77%) (32437/32840)\n",
      "Epoch: 3 | Batch_idx: 830 |  Loss: (0.0398) | Acc: (98.76%) (32829/33240)\n",
      "Epoch: 3 | Batch_idx: 840 |  Loss: (0.0397) | Acc: (98.77%) (33226/33640)\n",
      "Epoch: 3 | Batch_idx: 850 |  Loss: (0.0396) | Acc: (98.77%) (33620/34040)\n",
      "Epoch: 3 | Batch_idx: 860 |  Loss: (0.0395) | Acc: (98.77%) (34016/34440)\n",
      "Epoch: 3 | Batch_idx: 870 |  Loss: (0.0392) | Acc: (98.77%) (34413/34840)\n",
      "Epoch: 3 | Batch_idx: 880 |  Loss: (0.0392) | Acc: (98.78%) (34810/35240)\n",
      "Epoch: 3 | Batch_idx: 890 |  Loss: (0.0389) | Acc: (98.79%) (35208/35640)\n",
      "Epoch: 3 | Batch_idx: 900 |  Loss: (0.0389) | Acc: (98.79%) (35604/36040)\n",
      "Epoch: 3 | Batch_idx: 910 |  Loss: (0.0388) | Acc: (98.79%) (35998/36440)\n",
      "Epoch: 3 | Batch_idx: 920 |  Loss: (0.0392) | Acc: (98.77%) (36388/36840)\n",
      "Epoch: 3 | Batch_idx: 930 |  Loss: (0.0391) | Acc: (98.78%) (36785/37240)\n",
      "Epoch: 3 | Batch_idx: 940 |  Loss: (0.0390) | Acc: (98.78%) (37181/37640)\n",
      "Epoch: 3 | Batch_idx: 950 |  Loss: (0.0391) | Acc: (98.77%) (37573/38040)\n",
      "Epoch: 3 | Batch_idx: 960 |  Loss: (0.0390) | Acc: (98.77%) (37968/38440)\n",
      "Epoch: 3 | Batch_idx: 970 |  Loss: (0.0390) | Acc: (98.77%) (38361/38840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Batch_idx: 980 |  Loss: (0.0390) | Acc: (98.77%) (38756/39240)\n",
      "Epoch: 3 | Batch_idx: 990 |  Loss: (0.0390) | Acc: (98.77%) (39151/39640)\n",
      "Epoch: 3 | Batch_idx: 1000 |  Loss: (0.0389) | Acc: (98.77%) (39548/40040)\n",
      "Epoch: 3 | Batch_idx: 1010 |  Loss: (0.0389) | Acc: (98.77%) (39943/40440)\n",
      "Epoch: 3 | Batch_idx: 1020 |  Loss: (0.0388) | Acc: (98.77%) (40339/40840)\n",
      "Epoch: 3 | Batch_idx: 1030 |  Loss: (0.0392) | Acc: (98.77%) (40731/41240)\n",
      "Epoch: 3 | Batch_idx: 1040 |  Loss: (0.0391) | Acc: (98.77%) (41127/41640)\n",
      "Epoch: 3 | Batch_idx: 1050 |  Loss: (0.0391) | Acc: (98.77%) (41525/42040)\n",
      "Epoch: 3 | Batch_idx: 1060 |  Loss: (0.0389) | Acc: (98.78%) (41924/42440)\n",
      "Epoch: 3 | Batch_idx: 1070 |  Loss: (0.0387) | Acc: (98.79%) (42321/42840)\n",
      "Epoch: 3 | Batch_idx: 1080 |  Loss: (0.0386) | Acc: (98.78%) (42714/43240)\n",
      "Epoch: 3 | Batch_idx: 1090 |  Loss: (0.0387) | Acc: (98.78%) (43107/43640)\n",
      "Epoch: 3 | Batch_idx: 1100 |  Loss: (0.0389) | Acc: (98.77%) (43498/44040)\n",
      "Epoch: 3 | Batch_idx: 1110 |  Loss: (0.0389) | Acc: (98.77%) (43894/44440)\n",
      "Epoch: 3 | Batch_idx: 1120 |  Loss: (0.0387) | Acc: (98.78%) (44291/44840)\n",
      "Epoch: 3 | Batch_idx: 1130 |  Loss: (0.0386) | Acc: (98.77%) (44685/45240)\n",
      "Epoch: 3 | Batch_idx: 1140 |  Loss: (0.0391) | Acc: (98.77%) (45077/45640)\n",
      "Epoch: 3 | Batch_idx: 1150 |  Loss: (0.0391) | Acc: (98.77%) (45473/46040)\n",
      "Epoch: 3 | Batch_idx: 1160 |  Loss: (0.0394) | Acc: (98.76%) (45865/46440)\n",
      "Epoch: 3 | Batch_idx: 1170 |  Loss: (0.0392) | Acc: (98.77%) (46264/46840)\n",
      "Epoch: 3 | Batch_idx: 1180 |  Loss: (0.0391) | Acc: (98.77%) (46658/47240)\n",
      "Epoch: 3 | Batch_idx: 1190 |  Loss: (0.0390) | Acc: (98.77%) (47053/47640)\n",
      "Epoch: 3 | Batch_idx: 1200 |  Loss: (0.0389) | Acc: (98.77%) (47451/48040)\n",
      "Epoch: 3 | Batch_idx: 1210 |  Loss: (0.0389) | Acc: (98.77%) (47846/48440)\n",
      "Epoch: 3 | Batch_idx: 1220 |  Loss: (0.0391) | Acc: (98.76%) (48236/48840)\n",
      "Epoch: 3 | Batch_idx: 1230 |  Loss: (0.0391) | Acc: (98.76%) (48630/49240)\n",
      "Epoch: 3 | Batch_idx: 1240 |  Loss: (0.0392) | Acc: (98.76%) (49022/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1612) | Acc: (95.43%) (9543/10000)\n",
      "Epoch: 4 | Batch_idx: 0 |  Loss: (0.0096) | Acc: (100.00%) (40/40)\n",
      "Epoch: 4 | Batch_idx: 10 |  Loss: (0.0148) | Acc: (99.77%) (439/440)\n",
      "Epoch: 4 | Batch_idx: 20 |  Loss: (0.0174) | Acc: (99.40%) (835/840)\n",
      "Epoch: 4 | Batch_idx: 30 |  Loss: (0.0211) | Acc: (99.27%) (1231/1240)\n",
      "Epoch: 4 | Batch_idx: 40 |  Loss: (0.0197) | Acc: (99.33%) (1629/1640)\n",
      "Epoch: 4 | Batch_idx: 50 |  Loss: (0.0190) | Acc: (99.31%) (2026/2040)\n",
      "Epoch: 4 | Batch_idx: 60 |  Loss: (0.0173) | Acc: (99.43%) (2426/2440)\n",
      "Epoch: 4 | Batch_idx: 70 |  Loss: (0.0194) | Acc: (99.37%) (2822/2840)\n",
      "Epoch: 4 | Batch_idx: 80 |  Loss: (0.0210) | Acc: (99.35%) (3219/3240)\n",
      "Epoch: 4 | Batch_idx: 90 |  Loss: (0.0217) | Acc: (99.31%) (3615/3640)\n",
      "Epoch: 4 | Batch_idx: 100 |  Loss: (0.0229) | Acc: (99.31%) (4012/4040)\n",
      "Epoch: 4 | Batch_idx: 110 |  Loss: (0.0217) | Acc: (99.35%) (4411/4440)\n",
      "Epoch: 4 | Batch_idx: 120 |  Loss: (0.0217) | Acc: (99.34%) (4808/4840)\n",
      "Epoch: 4 | Batch_idx: 130 |  Loss: (0.0224) | Acc: (99.35%) (5206/5240)\n",
      "Epoch: 4 | Batch_idx: 140 |  Loss: (0.0228) | Acc: (99.31%) (5601/5640)\n",
      "Epoch: 4 | Batch_idx: 150 |  Loss: (0.0222) | Acc: (99.34%) (6000/6040)\n",
      "Epoch: 4 | Batch_idx: 160 |  Loss: (0.0222) | Acc: (99.33%) (6397/6440)\n",
      "Epoch: 4 | Batch_idx: 170 |  Loss: (0.0220) | Acc: (99.31%) (6793/6840)\n",
      "Epoch: 4 | Batch_idx: 180 |  Loss: (0.0223) | Acc: (99.31%) (7190/7240)\n",
      "Epoch: 4 | Batch_idx: 190 |  Loss: (0.0219) | Acc: (99.32%) (7588/7640)\n",
      "Epoch: 4 | Batch_idx: 200 |  Loss: (0.0216) | Acc: (99.34%) (7987/8040)\n",
      "Epoch: 4 | Batch_idx: 210 |  Loss: (0.0221) | Acc: (99.30%) (8381/8440)\n",
      "Epoch: 4 | Batch_idx: 220 |  Loss: (0.0220) | Acc: (99.29%) (8777/8840)\n",
      "Epoch: 4 | Batch_idx: 230 |  Loss: (0.0215) | Acc: (99.30%) (9175/9240)\n",
      "Epoch: 4 | Batch_idx: 240 |  Loss: (0.0216) | Acc: (99.30%) (9573/9640)\n",
      "Epoch: 4 | Batch_idx: 250 |  Loss: (0.0211) | Acc: (99.33%) (9973/10040)\n",
      "Epoch: 4 | Batch_idx: 260 |  Loss: (0.0217) | Acc: (99.32%) (10369/10440)\n",
      "Epoch: 4 | Batch_idx: 270 |  Loss: (0.0221) | Acc: (99.31%) (10765/10840)\n",
      "Epoch: 4 | Batch_idx: 280 |  Loss: (0.0224) | Acc: (99.31%) (11163/11240)\n",
      "Epoch: 4 | Batch_idx: 290 |  Loss: (0.0226) | Acc: (99.30%) (11559/11640)\n",
      "Epoch: 4 | Batch_idx: 300 |  Loss: (0.0235) | Acc: (99.27%) (11952/12040)\n",
      "Epoch: 4 | Batch_idx: 310 |  Loss: (0.0236) | Acc: (99.26%) (12348/12440)\n",
      "Epoch: 4 | Batch_idx: 320 |  Loss: (0.0239) | Acc: (99.25%) (12744/12840)\n",
      "Epoch: 4 | Batch_idx: 330 |  Loss: (0.0235) | Acc: (99.27%) (13143/13240)\n",
      "Epoch: 4 | Batch_idx: 340 |  Loss: (0.0233) | Acc: (99.27%) (13540/13640)\n",
      "Epoch: 4 | Batch_idx: 350 |  Loss: (0.0229) | Acc: (99.28%) (13939/14040)\n",
      "Epoch: 4 | Batch_idx: 360 |  Loss: (0.0240) | Acc: (99.25%) (14332/14440)\n",
      "Epoch: 4 | Batch_idx: 370 |  Loss: (0.0238) | Acc: (99.24%) (14727/14840)\n",
      "Epoch: 4 | Batch_idx: 380 |  Loss: (0.0238) | Acc: (99.25%) (15125/15240)\n",
      "Epoch: 4 | Batch_idx: 390 |  Loss: (0.0235) | Acc: (99.26%) (15524/15640)\n",
      "Epoch: 4 | Batch_idx: 400 |  Loss: (0.0236) | Acc: (99.25%) (15920/16040)\n",
      "Epoch: 4 | Batch_idx: 410 |  Loss: (0.0235) | Acc: (99.26%) (16318/16440)\n",
      "Epoch: 4 | Batch_idx: 420 |  Loss: (0.0236) | Acc: (99.25%) (16714/16840)\n",
      "Epoch: 4 | Batch_idx: 430 |  Loss: (0.0237) | Acc: (99.26%) (17112/17240)\n",
      "Epoch: 4 | Batch_idx: 440 |  Loss: (0.0237) | Acc: (99.25%) (17508/17640)\n",
      "Epoch: 4 | Batch_idx: 450 |  Loss: (0.0236) | Acc: (99.26%) (17907/18040)\n",
      "Epoch: 4 | Batch_idx: 460 |  Loss: (0.0235) | Acc: (99.26%) (18304/18440)\n",
      "Epoch: 4 | Batch_idx: 470 |  Loss: (0.0235) | Acc: (99.25%) (18698/18840)\n",
      "Epoch: 4 | Batch_idx: 480 |  Loss: (0.0237) | Acc: (99.24%) (19094/19240)\n",
      "Epoch: 4 | Batch_idx: 490 |  Loss: (0.0243) | Acc: (99.24%) (19490/19640)\n",
      "Epoch: 4 | Batch_idx: 500 |  Loss: (0.0244) | Acc: (99.23%) (19885/20040)\n",
      "Epoch: 4 | Batch_idx: 510 |  Loss: (0.0246) | Acc: (99.22%) (20281/20440)\n",
      "Epoch: 4 | Batch_idx: 520 |  Loss: (0.0244) | Acc: (99.23%) (20679/20840)\n",
      "Epoch: 4 | Batch_idx: 530 |  Loss: (0.0244) | Acc: (99.23%) (21076/21240)\n",
      "Epoch: 4 | Batch_idx: 540 |  Loss: (0.0244) | Acc: (99.22%) (21471/21640)\n",
      "Epoch: 4 | Batch_idx: 550 |  Loss: (0.0244) | Acc: (99.22%) (21869/22040)\n",
      "Epoch: 4 | Batch_idx: 560 |  Loss: (0.0247) | Acc: (99.22%) (22265/22440)\n",
      "Epoch: 4 | Batch_idx: 570 |  Loss: (0.0248) | Acc: (99.21%) (22660/22840)\n",
      "Epoch: 4 | Batch_idx: 580 |  Loss: (0.0246) | Acc: (99.23%) (23060/23240)\n",
      "Epoch: 4 | Batch_idx: 590 |  Loss: (0.0247) | Acc: (99.23%) (23457/23640)\n",
      "Epoch: 4 | Batch_idx: 600 |  Loss: (0.0251) | Acc: (99.22%) (23853/24040)\n",
      "Epoch: 4 | Batch_idx: 610 |  Loss: (0.0250) | Acc: (99.23%) (24251/24440)\n",
      "Epoch: 4 | Batch_idx: 620 |  Loss: (0.0251) | Acc: (99.24%) (24650/24840)\n",
      "Epoch: 4 | Batch_idx: 630 |  Loss: (0.0252) | Acc: (99.23%) (25046/25240)\n",
      "Epoch: 4 | Batch_idx: 640 |  Loss: (0.0255) | Acc: (99.22%) (25440/25640)\n",
      "Epoch: 4 | Batch_idx: 650 |  Loss: (0.0260) | Acc: (99.21%) (25834/26040)\n",
      "Epoch: 4 | Batch_idx: 660 |  Loss: (0.0260) | Acc: (99.21%) (26231/26440)\n",
      "Epoch: 4 | Batch_idx: 670 |  Loss: (0.0260) | Acc: (99.20%) (26625/26840)\n",
      "Epoch: 4 | Batch_idx: 680 |  Loss: (0.0259) | Acc: (99.21%) (27024/27240)\n",
      "Epoch: 4 | Batch_idx: 690 |  Loss: (0.0259) | Acc: (99.20%) (27420/27640)\n",
      "Epoch: 4 | Batch_idx: 700 |  Loss: (0.0258) | Acc: (99.21%) (27818/28040)\n",
      "Epoch: 4 | Batch_idx: 710 |  Loss: (0.0260) | Acc: (99.21%) (28215/28440)\n",
      "Epoch: 4 | Batch_idx: 720 |  Loss: (0.0261) | Acc: (99.20%) (28610/28840)\n",
      "Epoch: 4 | Batch_idx: 730 |  Loss: (0.0259) | Acc: (99.21%) (29009/29240)\n",
      "Epoch: 4 | Batch_idx: 740 |  Loss: (0.0261) | Acc: (99.21%) (29406/29640)\n",
      "Epoch: 4 | Batch_idx: 750 |  Loss: (0.0260) | Acc: (99.21%) (29804/30040)\n",
      "Epoch: 4 | Batch_idx: 760 |  Loss: (0.0257) | Acc: (99.22%) (30204/30440)\n",
      "Epoch: 4 | Batch_idx: 770 |  Loss: (0.0258) | Acc: (99.23%) (30602/30840)\n",
      "Epoch: 4 | Batch_idx: 780 |  Loss: (0.0257) | Acc: (99.23%) (30999/31240)\n",
      "Epoch: 4 | Batch_idx: 790 |  Loss: (0.0258) | Acc: (99.22%) (31393/31640)\n",
      "Epoch: 4 | Batch_idx: 800 |  Loss: (0.0258) | Acc: (99.23%) (31792/32040)\n",
      "Epoch: 4 | Batch_idx: 810 |  Loss: (0.0256) | Acc: (99.23%) (32190/32440)\n",
      "Epoch: 4 | Batch_idx: 820 |  Loss: (0.0254) | Acc: (99.24%) (32589/32840)\n",
      "Epoch: 4 | Batch_idx: 830 |  Loss: (0.0255) | Acc: (99.24%) (32986/33240)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Batch_idx: 840 |  Loss: (0.0257) | Acc: (99.23%) (33381/33640)\n",
      "Epoch: 4 | Batch_idx: 850 |  Loss: (0.0257) | Acc: (99.23%) (33777/34040)\n",
      "Epoch: 4 | Batch_idx: 860 |  Loss: (0.0257) | Acc: (99.22%) (34173/34440)\n",
      "Epoch: 4 | Batch_idx: 870 |  Loss: (0.0259) | Acc: (99.22%) (34568/34840)\n",
      "Epoch: 4 | Batch_idx: 880 |  Loss: (0.0258) | Acc: (99.22%) (34965/35240)\n",
      "Epoch: 4 | Batch_idx: 890 |  Loss: (0.0260) | Acc: (99.22%) (35361/35640)\n",
      "Epoch: 4 | Batch_idx: 900 |  Loss: (0.0261) | Acc: (99.21%) (35754/36040)\n",
      "Epoch: 4 | Batch_idx: 910 |  Loss: (0.0265) | Acc: (99.20%) (36148/36440)\n",
      "Epoch: 4 | Batch_idx: 920 |  Loss: (0.0265) | Acc: (99.20%) (36544/36840)\n",
      "Epoch: 4 | Batch_idx: 930 |  Loss: (0.0265) | Acc: (99.20%) (36941/37240)\n",
      "Epoch: 4 | Batch_idx: 940 |  Loss: (0.0264) | Acc: (99.20%) (37340/37640)\n",
      "Epoch: 4 | Batch_idx: 950 |  Loss: (0.0265) | Acc: (99.20%) (37736/38040)\n",
      "Epoch: 4 | Batch_idx: 960 |  Loss: (0.0266) | Acc: (99.20%) (38131/38440)\n",
      "Epoch: 4 | Batch_idx: 970 |  Loss: (0.0268) | Acc: (99.18%) (38523/38840)\n",
      "Epoch: 4 | Batch_idx: 980 |  Loss: (0.0269) | Acc: (99.18%) (38919/39240)\n",
      "Epoch: 4 | Batch_idx: 990 |  Loss: (0.0268) | Acc: (99.18%) (39316/39640)\n",
      "Epoch: 4 | Batch_idx: 1000 |  Loss: (0.0267) | Acc: (99.18%) (39713/40040)\n",
      "Epoch: 4 | Batch_idx: 1010 |  Loss: (0.0266) | Acc: (99.19%) (40111/40440)\n",
      "Epoch: 4 | Batch_idx: 1020 |  Loss: (0.0267) | Acc: (99.18%) (40507/40840)\n",
      "Epoch: 4 | Batch_idx: 1030 |  Loss: (0.0267) | Acc: (99.19%) (40904/41240)\n",
      "Epoch: 4 | Batch_idx: 1040 |  Loss: (0.0266) | Acc: (99.19%) (41302/41640)\n",
      "Epoch: 4 | Batch_idx: 1050 |  Loss: (0.0267) | Acc: (99.18%) (41697/42040)\n",
      "Epoch: 4 | Batch_idx: 1060 |  Loss: (0.0266) | Acc: (99.19%) (42097/42440)\n",
      "Epoch: 4 | Batch_idx: 1070 |  Loss: (0.0267) | Acc: (99.19%) (42491/42840)\n",
      "Epoch: 4 | Batch_idx: 1080 |  Loss: (0.0266) | Acc: (99.18%) (42887/43240)\n",
      "Epoch: 4 | Batch_idx: 1090 |  Loss: (0.0266) | Acc: (99.18%) (43284/43640)\n",
      "Epoch: 4 | Batch_idx: 1100 |  Loss: (0.0268) | Acc: (99.17%) (43676/44040)\n",
      "Epoch: 4 | Batch_idx: 1110 |  Loss: (0.0266) | Acc: (99.18%) (44074/44440)\n",
      "Epoch: 4 | Batch_idx: 1120 |  Loss: (0.0266) | Acc: (99.18%) (44471/44840)\n",
      "Epoch: 4 | Batch_idx: 1130 |  Loss: (0.0268) | Acc: (99.18%) (44868/45240)\n",
      "Epoch: 4 | Batch_idx: 1140 |  Loss: (0.0269) | Acc: (99.17%) (45260/45640)\n",
      "Epoch: 4 | Batch_idx: 1150 |  Loss: (0.0268) | Acc: (99.17%) (45659/46040)\n",
      "Epoch: 4 | Batch_idx: 1160 |  Loss: (0.0269) | Acc: (99.17%) (46055/46440)\n",
      "Epoch: 4 | Batch_idx: 1170 |  Loss: (0.0270) | Acc: (99.17%) (46450/46840)\n",
      "Epoch: 4 | Batch_idx: 1180 |  Loss: (0.0270) | Acc: (99.17%) (46846/47240)\n",
      "Epoch: 4 | Batch_idx: 1190 |  Loss: (0.0269) | Acc: (99.17%) (47246/47640)\n",
      "Epoch: 4 | Batch_idx: 1200 |  Loss: (0.0269) | Acc: (99.17%) (47643/48040)\n",
      "Epoch: 4 | Batch_idx: 1210 |  Loss: (0.0268) | Acc: (99.17%) (48040/48440)\n",
      "Epoch: 4 | Batch_idx: 1220 |  Loss: (0.0269) | Acc: (99.17%) (48436/48840)\n",
      "Epoch: 4 | Batch_idx: 1230 |  Loss: (0.0269) | Acc: (99.17%) (48832/49240)\n",
      "Epoch: 4 | Batch_idx: 1240 |  Loss: (0.0268) | Acc: (99.18%) (49231/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1722) | Acc: (95.48%) (9548/10000)\n",
      "Epoch: 5 | Batch_idx: 0 |  Loss: (0.0037) | Acc: (100.00%) (40/40)\n",
      "Epoch: 5 | Batch_idx: 10 |  Loss: (0.0088) | Acc: (99.77%) (439/440)\n",
      "Epoch: 5 | Batch_idx: 20 |  Loss: (0.0094) | Acc: (99.88%) (839/840)\n",
      "Epoch: 5 | Batch_idx: 30 |  Loss: (0.0086) | Acc: (99.92%) (1239/1240)\n",
      "Epoch: 5 | Batch_idx: 40 |  Loss: (0.0101) | Acc: (99.76%) (1636/1640)\n",
      "Epoch: 5 | Batch_idx: 50 |  Loss: (0.0126) | Acc: (99.66%) (2033/2040)\n",
      "Epoch: 5 | Batch_idx: 60 |  Loss: (0.0120) | Acc: (99.67%) (2432/2440)\n",
      "Epoch: 5 | Batch_idx: 70 |  Loss: (0.0131) | Acc: (99.65%) (2830/2840)\n",
      "Epoch: 5 | Batch_idx: 80 |  Loss: (0.0127) | Acc: (99.66%) (3229/3240)\n",
      "Epoch: 5 | Batch_idx: 90 |  Loss: (0.0132) | Acc: (99.64%) (3627/3640)\n",
      "Epoch: 5 | Batch_idx: 100 |  Loss: (0.0126) | Acc: (99.68%) (4027/4040)\n",
      "Epoch: 5 | Batch_idx: 110 |  Loss: (0.0119) | Acc: (99.71%) (4427/4440)\n",
      "Epoch: 5 | Batch_idx: 120 |  Loss: (0.0120) | Acc: (99.67%) (4824/4840)\n",
      "Epoch: 5 | Batch_idx: 130 |  Loss: (0.0134) | Acc: (99.68%) (5223/5240)\n",
      "Epoch: 5 | Batch_idx: 140 |  Loss: (0.0138) | Acc: (99.66%) (5621/5640)\n",
      "Epoch: 5 | Batch_idx: 150 |  Loss: (0.0139) | Acc: (99.65%) (6019/6040)\n",
      "Epoch: 5 | Batch_idx: 160 |  Loss: (0.0143) | Acc: (99.66%) (6418/6440)\n",
      "Epoch: 5 | Batch_idx: 170 |  Loss: (0.0139) | Acc: (99.66%) (6817/6840)\n",
      "Epoch: 5 | Batch_idx: 180 |  Loss: (0.0141) | Acc: (99.65%) (7215/7240)\n",
      "Epoch: 5 | Batch_idx: 190 |  Loss: (0.0145) | Acc: (99.63%) (7612/7640)\n",
      "Epoch: 5 | Batch_idx: 200 |  Loss: (0.0149) | Acc: (99.60%) (8008/8040)\n",
      "Epoch: 5 | Batch_idx: 210 |  Loss: (0.0154) | Acc: (99.59%) (8405/8440)\n",
      "Epoch: 5 | Batch_idx: 220 |  Loss: (0.0153) | Acc: (99.58%) (8803/8840)\n",
      "Epoch: 5 | Batch_idx: 230 |  Loss: (0.0149) | Acc: (99.59%) (9202/9240)\n",
      "Epoch: 5 | Batch_idx: 240 |  Loss: (0.0145) | Acc: (99.61%) (9602/9640)\n",
      "Epoch: 5 | Batch_idx: 250 |  Loss: (0.0148) | Acc: (99.60%) (10000/10040)\n",
      "Epoch: 5 | Batch_idx: 260 |  Loss: (0.0147) | Acc: (99.60%) (10398/10440)\n",
      "Epoch: 5 | Batch_idx: 270 |  Loss: (0.0148) | Acc: (99.59%) (10796/10840)\n",
      "Epoch: 5 | Batch_idx: 280 |  Loss: (0.0149) | Acc: (99.59%) (11194/11240)\n",
      "Epoch: 5 | Batch_idx: 290 |  Loss: (0.0147) | Acc: (99.60%) (11593/11640)\n",
      "Epoch: 5 | Batch_idx: 300 |  Loss: (0.0145) | Acc: (99.61%) (11993/12040)\n",
      "Epoch: 5 | Batch_idx: 310 |  Loss: (0.0143) | Acc: (99.61%) (12392/12440)\n",
      "Epoch: 5 | Batch_idx: 320 |  Loss: (0.0142) | Acc: (99.61%) (12790/12840)\n",
      "Epoch: 5 | Batch_idx: 330 |  Loss: (0.0148) | Acc: (99.58%) (13185/13240)\n",
      "Epoch: 5 | Batch_idx: 340 |  Loss: (0.0154) | Acc: (99.57%) (13582/13640)\n",
      "Epoch: 5 | Batch_idx: 350 |  Loss: (0.0151) | Acc: (99.59%) (13982/14040)\n",
      "Epoch: 5 | Batch_idx: 360 |  Loss: (0.0149) | Acc: (99.59%) (14381/14440)\n",
      "Epoch: 5 | Batch_idx: 370 |  Loss: (0.0150) | Acc: (99.58%) (14778/14840)\n",
      "Epoch: 5 | Batch_idx: 380 |  Loss: (0.0149) | Acc: (99.58%) (15176/15240)\n",
      "Epoch: 5 | Batch_idx: 390 |  Loss: (0.0152) | Acc: (99.58%) (15574/15640)\n",
      "Epoch: 5 | Batch_idx: 400 |  Loss: (0.0155) | Acc: (99.56%) (15970/16040)\n",
      "Epoch: 5 | Batch_idx: 410 |  Loss: (0.0156) | Acc: (99.56%) (16368/16440)\n",
      "Epoch: 5 | Batch_idx: 420 |  Loss: (0.0156) | Acc: (99.56%) (16766/16840)\n",
      "Epoch: 5 | Batch_idx: 430 |  Loss: (0.0158) | Acc: (99.56%) (17164/17240)\n",
      "Epoch: 5 | Batch_idx: 440 |  Loss: (0.0157) | Acc: (99.56%) (17563/17640)\n",
      "Epoch: 5 | Batch_idx: 450 |  Loss: (0.0155) | Acc: (99.57%) (17963/18040)\n",
      "Epoch: 5 | Batch_idx: 460 |  Loss: (0.0154) | Acc: (99.57%) (18361/18440)\n",
      "Epoch: 5 | Batch_idx: 470 |  Loss: (0.0154) | Acc: (99.56%) (18758/18840)\n",
      "Epoch: 5 | Batch_idx: 480 |  Loss: (0.0155) | Acc: (99.56%) (19156/19240)\n",
      "Epoch: 5 | Batch_idx: 490 |  Loss: (0.0153) | Acc: (99.57%) (19555/19640)\n",
      "Epoch: 5 | Batch_idx: 500 |  Loss: (0.0156) | Acc: (99.56%) (19952/20040)\n",
      "Epoch: 5 | Batch_idx: 510 |  Loss: (0.0155) | Acc: (99.56%) (20351/20440)\n",
      "Epoch: 5 | Batch_idx: 520 |  Loss: (0.0155) | Acc: (99.57%) (20750/20840)\n",
      "Epoch: 5 | Batch_idx: 530 |  Loss: (0.0154) | Acc: (99.57%) (21149/21240)\n",
      "Epoch: 5 | Batch_idx: 540 |  Loss: (0.0154) | Acc: (99.57%) (21546/21640)\n",
      "Epoch: 5 | Batch_idx: 550 |  Loss: (0.0156) | Acc: (99.56%) (21944/22040)\n",
      "Epoch: 5 | Batch_idx: 560 |  Loss: (0.0154) | Acc: (99.57%) (22344/22440)\n",
      "Epoch: 5 | Batch_idx: 570 |  Loss: (0.0155) | Acc: (99.55%) (22738/22840)\n",
      "Epoch: 5 | Batch_idx: 580 |  Loss: (0.0157) | Acc: (99.54%) (23133/23240)\n",
      "Epoch: 5 | Batch_idx: 590 |  Loss: (0.0159) | Acc: (99.53%) (23530/23640)\n",
      "Epoch: 5 | Batch_idx: 600 |  Loss: (0.0157) | Acc: (99.54%) (23929/24040)\n",
      "Epoch: 5 | Batch_idx: 610 |  Loss: (0.0156) | Acc: (99.54%) (24328/24440)\n",
      "Epoch: 5 | Batch_idx: 620 |  Loss: (0.0156) | Acc: (99.54%) (24726/24840)\n",
      "Epoch: 5 | Batch_idx: 630 |  Loss: (0.0155) | Acc: (99.55%) (25126/25240)\n",
      "Epoch: 5 | Batch_idx: 640 |  Loss: (0.0156) | Acc: (99.55%) (25524/25640)\n",
      "Epoch: 5 | Batch_idx: 650 |  Loss: (0.0156) | Acc: (99.55%) (25922/26040)\n",
      "Epoch: 5 | Batch_idx: 660 |  Loss: (0.0157) | Acc: (99.54%) (26319/26440)\n",
      "Epoch: 5 | Batch_idx: 670 |  Loss: (0.0156) | Acc: (99.54%) (26717/26840)\n",
      "Epoch: 5 | Batch_idx: 680 |  Loss: (0.0155) | Acc: (99.55%) (27117/27240)\n",
      "Epoch: 5 | Batch_idx: 690 |  Loss: (0.0156) | Acc: (99.55%) (27516/27640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Batch_idx: 700 |  Loss: (0.0157) | Acc: (99.55%) (27913/28040)\n",
      "Epoch: 5 | Batch_idx: 710 |  Loss: (0.0158) | Acc: (99.54%) (28309/28440)\n",
      "Epoch: 5 | Batch_idx: 720 |  Loss: (0.0158) | Acc: (99.54%) (28707/28840)\n",
      "Epoch: 5 | Batch_idx: 730 |  Loss: (0.0158) | Acc: (99.54%) (29105/29240)\n",
      "Epoch: 5 | Batch_idx: 740 |  Loss: (0.0158) | Acc: (99.54%) (29503/29640)\n",
      "Epoch: 5 | Batch_idx: 750 |  Loss: (0.0157) | Acc: (99.54%) (29901/30040)\n",
      "Epoch: 5 | Batch_idx: 760 |  Loss: (0.0156) | Acc: (99.54%) (30300/30440)\n",
      "Epoch: 5 | Batch_idx: 770 |  Loss: (0.0157) | Acc: (99.53%) (30696/30840)\n",
      "Epoch: 5 | Batch_idx: 780 |  Loss: (0.0157) | Acc: (99.53%) (31092/31240)\n",
      "Epoch: 5 | Batch_idx: 790 |  Loss: (0.0159) | Acc: (99.51%) (31486/31640)\n",
      "Epoch: 5 | Batch_idx: 800 |  Loss: (0.0161) | Acc: (99.50%) (31881/32040)\n",
      "Epoch: 5 | Batch_idx: 810 |  Loss: (0.0161) | Acc: (99.50%) (32279/32440)\n",
      "Epoch: 5 | Batch_idx: 820 |  Loss: (0.0160) | Acc: (99.50%) (32677/32840)\n",
      "Epoch: 5 | Batch_idx: 830 |  Loss: (0.0162) | Acc: (99.50%) (33073/33240)\n",
      "Epoch: 5 | Batch_idx: 840 |  Loss: (0.0163) | Acc: (99.49%) (33467/33640)\n",
      "Epoch: 5 | Batch_idx: 850 |  Loss: (0.0162) | Acc: (99.49%) (33866/34040)\n",
      "Epoch: 5 | Batch_idx: 860 |  Loss: (0.0163) | Acc: (99.48%) (34262/34440)\n",
      "Epoch: 5 | Batch_idx: 870 |  Loss: (0.0163) | Acc: (99.48%) (34660/34840)\n",
      "Epoch: 5 | Batch_idx: 880 |  Loss: (0.0164) | Acc: (99.48%) (35058/35240)\n",
      "Epoch: 5 | Batch_idx: 890 |  Loss: (0.0164) | Acc: (99.48%) (35454/35640)\n",
      "Epoch: 5 | Batch_idx: 900 |  Loss: (0.0163) | Acc: (99.48%) (35854/36040)\n",
      "Epoch: 5 | Batch_idx: 910 |  Loss: (0.0162) | Acc: (99.49%) (36254/36440)\n",
      "Epoch: 5 | Batch_idx: 920 |  Loss: (0.0164) | Acc: (99.48%) (36648/36840)\n",
      "Epoch: 5 | Batch_idx: 930 |  Loss: (0.0163) | Acc: (99.48%) (37047/37240)\n",
      "Epoch: 5 | Batch_idx: 940 |  Loss: (0.0163) | Acc: (99.48%) (37446/37640)\n",
      "Epoch: 5 | Batch_idx: 950 |  Loss: (0.0162) | Acc: (99.49%) (37845/38040)\n",
      "Epoch: 5 | Batch_idx: 960 |  Loss: (0.0162) | Acc: (99.49%) (38244/38440)\n",
      "Epoch: 5 | Batch_idx: 970 |  Loss: (0.0161) | Acc: (99.49%) (38643/38840)\n",
      "Epoch: 5 | Batch_idx: 980 |  Loss: (0.0161) | Acc: (99.50%) (39042/39240)\n",
      "Epoch: 5 | Batch_idx: 990 |  Loss: (0.0161) | Acc: (99.50%) (39440/39640)\n",
      "Epoch: 5 | Batch_idx: 1000 |  Loss: (0.0163) | Acc: (99.49%) (39835/40040)\n",
      "Epoch: 5 | Batch_idx: 1010 |  Loss: (0.0162) | Acc: (99.49%) (40234/40440)\n",
      "Epoch: 5 | Batch_idx: 1020 |  Loss: (0.0161) | Acc: (99.49%) (40633/40840)\n",
      "Epoch: 5 | Batch_idx: 1030 |  Loss: (0.0163) | Acc: (99.49%) (41029/41240)\n",
      "Epoch: 5 | Batch_idx: 1040 |  Loss: (0.0162) | Acc: (99.49%) (41428/41640)\n",
      "Epoch: 5 | Batch_idx: 1050 |  Loss: (0.0162) | Acc: (99.49%) (41826/42040)\n",
      "Epoch: 5 | Batch_idx: 1060 |  Loss: (0.0162) | Acc: (99.49%) (42225/42440)\n",
      "Epoch: 5 | Batch_idx: 1070 |  Loss: (0.0161) | Acc: (99.49%) (42623/42840)\n",
      "Epoch: 5 | Batch_idx: 1080 |  Loss: (0.0160) | Acc: (99.50%) (43023/43240)\n",
      "Epoch: 5 | Batch_idx: 1090 |  Loss: (0.0161) | Acc: (99.50%) (43421/43640)\n",
      "Epoch: 5 | Batch_idx: 1100 |  Loss: (0.0160) | Acc: (99.50%) (43821/44040)\n",
      "Epoch: 5 | Batch_idx: 1110 |  Loss: (0.0161) | Acc: (99.50%) (44219/44440)\n",
      "Epoch: 5 | Batch_idx: 1120 |  Loss: (0.0162) | Acc: (99.50%) (44617/44840)\n",
      "Epoch: 5 | Batch_idx: 1130 |  Loss: (0.0161) | Acc: (99.51%) (45017/45240)\n",
      "Epoch: 5 | Batch_idx: 1140 |  Loss: (0.0162) | Acc: (99.50%) (45410/45640)\n",
      "Epoch: 5 | Batch_idx: 1150 |  Loss: (0.0162) | Acc: (99.49%) (45807/46040)\n",
      "Epoch: 5 | Batch_idx: 1160 |  Loss: (0.0162) | Acc: (99.49%) (46205/46440)\n",
      "Epoch: 5 | Batch_idx: 1170 |  Loss: (0.0162) | Acc: (99.49%) (46602/46840)\n",
      "Epoch: 5 | Batch_idx: 1180 |  Loss: (0.0163) | Acc: (99.49%) (47000/47240)\n",
      "Epoch: 5 | Batch_idx: 1190 |  Loss: (0.0163) | Acc: (99.49%) (47398/47640)\n",
      "Epoch: 5 | Batch_idx: 1200 |  Loss: (0.0162) | Acc: (99.49%) (47795/48040)\n",
      "Epoch: 5 | Batch_idx: 1210 |  Loss: (0.0161) | Acc: (99.49%) (48195/48440)\n",
      "Epoch: 5 | Batch_idx: 1220 |  Loss: (0.0162) | Acc: (99.49%) (48591/48840)\n",
      "Epoch: 5 | Batch_idx: 1230 |  Loss: (0.0162) | Acc: (99.49%) (48988/49240)\n",
      "Epoch: 5 | Batch_idx: 1240 |  Loss: (0.0161) | Acc: (99.49%) (49386/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1863) | Acc: (95.51%) (9551/10000)\n",
      "Epoch: 6 | Batch_idx: 0 |  Loss: (0.0034) | Acc: (100.00%) (40/40)\n",
      "Epoch: 6 | Batch_idx: 10 |  Loss: (0.0152) | Acc: (99.55%) (438/440)\n",
      "Epoch: 6 | Batch_idx: 20 |  Loss: (0.0117) | Acc: (99.64%) (837/840)\n",
      "Epoch: 6 | Batch_idx: 30 |  Loss: (0.0098) | Acc: (99.68%) (1236/1240)\n",
      "Epoch: 6 | Batch_idx: 40 |  Loss: (0.0108) | Acc: (99.63%) (1634/1640)\n",
      "Epoch: 6 | Batch_idx: 50 |  Loss: (0.0104) | Acc: (99.66%) (2033/2040)\n",
      "Epoch: 6 | Batch_idx: 60 |  Loss: (0.0102) | Acc: (99.63%) (2431/2440)\n",
      "Epoch: 6 | Batch_idx: 70 |  Loss: (0.0111) | Acc: (99.61%) (2829/2840)\n",
      "Epoch: 6 | Batch_idx: 80 |  Loss: (0.0109) | Acc: (99.60%) (3227/3240)\n",
      "Epoch: 6 | Batch_idx: 90 |  Loss: (0.0104) | Acc: (99.64%) (3627/3640)\n",
      "Epoch: 6 | Batch_idx: 100 |  Loss: (0.0099) | Acc: (99.65%) (4026/4040)\n",
      "Epoch: 6 | Batch_idx: 110 |  Loss: (0.0098) | Acc: (99.64%) (4424/4440)\n",
      "Epoch: 6 | Batch_idx: 120 |  Loss: (0.0118) | Acc: (99.57%) (4819/4840)\n",
      "Epoch: 6 | Batch_idx: 130 |  Loss: (0.0122) | Acc: (99.52%) (5215/5240)\n",
      "Epoch: 6 | Batch_idx: 140 |  Loss: (0.0115) | Acc: (99.56%) (5615/5640)\n",
      "Epoch: 6 | Batch_idx: 150 |  Loss: (0.0116) | Acc: (99.52%) (6011/6040)\n",
      "Epoch: 6 | Batch_idx: 160 |  Loss: (0.0116) | Acc: (99.53%) (6410/6440)\n",
      "Epoch: 6 | Batch_idx: 170 |  Loss: (0.0111) | Acc: (99.56%) (6810/6840)\n",
      "Epoch: 6 | Batch_idx: 180 |  Loss: (0.0112) | Acc: (99.54%) (7207/7240)\n",
      "Epoch: 6 | Batch_idx: 190 |  Loss: (0.0108) | Acc: (99.57%) (7607/7640)\n",
      "Epoch: 6 | Batch_idx: 200 |  Loss: (0.0109) | Acc: (99.56%) (8005/8040)\n",
      "Epoch: 6 | Batch_idx: 210 |  Loss: (0.0120) | Acc: (99.54%) (8401/8440)\n",
      "Epoch: 6 | Batch_idx: 220 |  Loss: (0.0123) | Acc: (99.54%) (8799/8840)\n",
      "Epoch: 6 | Batch_idx: 230 |  Loss: (0.0120) | Acc: (99.56%) (9199/9240)\n",
      "Epoch: 6 | Batch_idx: 240 |  Loss: (0.0117) | Acc: (99.56%) (9598/9640)\n",
      "Epoch: 6 | Batch_idx: 250 |  Loss: (0.0115) | Acc: (99.58%) (9998/10040)\n",
      "Epoch: 6 | Batch_idx: 260 |  Loss: (0.0115) | Acc: (99.58%) (10396/10440)\n",
      "Epoch: 6 | Batch_idx: 270 |  Loss: (0.0115) | Acc: (99.58%) (10794/10840)\n",
      "Epoch: 6 | Batch_idx: 280 |  Loss: (0.0112) | Acc: (99.59%) (11194/11240)\n",
      "Epoch: 6 | Batch_idx: 290 |  Loss: (0.0118) | Acc: (99.56%) (11589/11640)\n",
      "Epoch: 6 | Batch_idx: 300 |  Loss: (0.0131) | Acc: (99.53%) (11983/12040)\n",
      "Epoch: 6 | Batch_idx: 310 |  Loss: (0.0135) | Acc: (99.51%) (12379/12440)\n",
      "Epoch: 6 | Batch_idx: 320 |  Loss: (0.0137) | Acc: (99.50%) (12776/12840)\n",
      "Epoch: 6 | Batch_idx: 330 |  Loss: (0.0135) | Acc: (99.52%) (13176/13240)\n",
      "Epoch: 6 | Batch_idx: 340 |  Loss: (0.0137) | Acc: (99.52%) (13575/13640)\n",
      "Epoch: 6 | Batch_idx: 350 |  Loss: (0.0136) | Acc: (99.52%) (13972/14040)\n",
      "Epoch: 6 | Batch_idx: 360 |  Loss: (0.0138) | Acc: (99.52%) (14370/14440)\n",
      "Epoch: 6 | Batch_idx: 370 |  Loss: (0.0135) | Acc: (99.53%) (14770/14840)\n",
      "Epoch: 6 | Batch_idx: 380 |  Loss: (0.0133) | Acc: (99.53%) (15169/15240)\n",
      "Epoch: 6 | Batch_idx: 390 |  Loss: (0.0131) | Acc: (99.54%) (15568/15640)\n",
      "Epoch: 6 | Batch_idx: 400 |  Loss: (0.0129) | Acc: (99.55%) (15968/16040)\n",
      "Epoch: 6 | Batch_idx: 410 |  Loss: (0.0127) | Acc: (99.56%) (16367/16440)\n",
      "Epoch: 6 | Batch_idx: 420 |  Loss: (0.0126) | Acc: (99.57%) (16767/16840)\n",
      "Epoch: 6 | Batch_idx: 430 |  Loss: (0.0127) | Acc: (99.56%) (17164/17240)\n",
      "Epoch: 6 | Batch_idx: 440 |  Loss: (0.0131) | Acc: (99.54%) (17559/17640)\n",
      "Epoch: 6 | Batch_idx: 450 |  Loss: (0.0129) | Acc: (99.55%) (17959/18040)\n",
      "Epoch: 6 | Batch_idx: 460 |  Loss: (0.0128) | Acc: (99.56%) (18358/18440)\n",
      "Epoch: 6 | Batch_idx: 470 |  Loss: (0.0126) | Acc: (99.56%) (18757/18840)\n",
      "Epoch: 6 | Batch_idx: 480 |  Loss: (0.0126) | Acc: (99.56%) (19155/19240)\n",
      "Epoch: 6 | Batch_idx: 490 |  Loss: (0.0126) | Acc: (99.55%) (19552/19640)\n",
      "Epoch: 6 | Batch_idx: 500 |  Loss: (0.0126) | Acc: (99.55%) (19950/20040)\n",
      "Epoch: 6 | Batch_idx: 510 |  Loss: (0.0124) | Acc: (99.56%) (20350/20440)\n",
      "Epoch: 6 | Batch_idx: 520 |  Loss: (0.0122) | Acc: (99.57%) (20750/20840)\n",
      "Epoch: 6 | Batch_idx: 530 |  Loss: (0.0121) | Acc: (99.57%) (21149/21240)\n",
      "Epoch: 6 | Batch_idx: 540 |  Loss: (0.0120) | Acc: (99.58%) (21549/21640)\n",
      "Epoch: 6 | Batch_idx: 550 |  Loss: (0.0120) | Acc: (99.58%) (21947/22040)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Batch_idx: 560 |  Loss: (0.0119) | Acc: (99.59%) (22347/22440)\n",
      "Epoch: 6 | Batch_idx: 570 |  Loss: (0.0120) | Acc: (99.59%) (22746/22840)\n",
      "Epoch: 6 | Batch_idx: 580 |  Loss: (0.0119) | Acc: (99.59%) (23145/23240)\n",
      "Epoch: 6 | Batch_idx: 590 |  Loss: (0.0118) | Acc: (99.59%) (23544/23640)\n",
      "Epoch: 6 | Batch_idx: 600 |  Loss: (0.0118) | Acc: (99.60%) (23943/24040)\n",
      "Epoch: 6 | Batch_idx: 610 |  Loss: (0.0117) | Acc: (99.60%) (24343/24440)\n",
      "Epoch: 6 | Batch_idx: 620 |  Loss: (0.0117) | Acc: (99.61%) (24743/24840)\n",
      "Epoch: 6 | Batch_idx: 630 |  Loss: (0.0116) | Acc: (99.61%) (25142/25240)\n",
      "Epoch: 6 | Batch_idx: 640 |  Loss: (0.0119) | Acc: (99.61%) (25539/25640)\n",
      "Epoch: 6 | Batch_idx: 650 |  Loss: (0.0119) | Acc: (99.60%) (25937/26040)\n",
      "Epoch: 6 | Batch_idx: 660 |  Loss: (0.0118) | Acc: (99.61%) (26337/26440)\n",
      "Epoch: 6 | Batch_idx: 670 |  Loss: (0.0118) | Acc: (99.61%) (26736/26840)\n",
      "Epoch: 6 | Batch_idx: 680 |  Loss: (0.0118) | Acc: (99.61%) (27135/27240)\n",
      "Epoch: 6 | Batch_idx: 690 |  Loss: (0.0118) | Acc: (99.62%) (27534/27640)\n",
      "Epoch: 6 | Batch_idx: 700 |  Loss: (0.0117) | Acc: (99.62%) (27934/28040)\n",
      "Epoch: 6 | Batch_idx: 710 |  Loss: (0.0117) | Acc: (99.62%) (28331/28440)\n",
      "Epoch: 6 | Batch_idx: 720 |  Loss: (0.0118) | Acc: (99.62%) (28729/28840)\n",
      "Epoch: 6 | Batch_idx: 730 |  Loss: (0.0117) | Acc: (99.62%) (29128/29240)\n",
      "Epoch: 6 | Batch_idx: 740 |  Loss: (0.0118) | Acc: (99.62%) (29526/29640)\n",
      "Epoch: 6 | Batch_idx: 750 |  Loss: (0.0119) | Acc: (99.61%) (29924/30040)\n",
      "Epoch: 6 | Batch_idx: 760 |  Loss: (0.0117) | Acc: (99.62%) (30324/30440)\n",
      "Epoch: 6 | Batch_idx: 770 |  Loss: (0.0117) | Acc: (99.62%) (30723/30840)\n",
      "Epoch: 6 | Batch_idx: 780 |  Loss: (0.0116) | Acc: (99.63%) (31123/31240)\n",
      "Epoch: 6 | Batch_idx: 790 |  Loss: (0.0115) | Acc: (99.63%) (31523/31640)\n",
      "Epoch: 6 | Batch_idx: 800 |  Loss: (0.0114) | Acc: (99.63%) (31921/32040)\n",
      "Epoch: 6 | Batch_idx: 810 |  Loss: (0.0114) | Acc: (99.63%) (32320/32440)\n",
      "Epoch: 6 | Batch_idx: 820 |  Loss: (0.0114) | Acc: (99.63%) (32719/32840)\n",
      "Epoch: 6 | Batch_idx: 830 |  Loss: (0.0113) | Acc: (99.64%) (33119/33240)\n",
      "Epoch: 6 | Batch_idx: 840 |  Loss: (0.0113) | Acc: (99.64%) (33518/33640)\n",
      "Epoch: 6 | Batch_idx: 850 |  Loss: (0.0112) | Acc: (99.64%) (33916/34040)\n",
      "Epoch: 6 | Batch_idx: 860 |  Loss: (0.0112) | Acc: (99.64%) (34316/34440)\n",
      "Epoch: 6 | Batch_idx: 870 |  Loss: (0.0111) | Acc: (99.64%) (34715/34840)\n",
      "Epoch: 6 | Batch_idx: 880 |  Loss: (0.0111) | Acc: (99.64%) (35114/35240)\n",
      "Epoch: 6 | Batch_idx: 890 |  Loss: (0.0113) | Acc: (99.63%) (35508/35640)\n",
      "Epoch: 6 | Batch_idx: 900 |  Loss: (0.0115) | Acc: (99.62%) (35904/36040)\n",
      "Epoch: 6 | Batch_idx: 910 |  Loss: (0.0115) | Acc: (99.62%) (36302/36440)\n",
      "Epoch: 6 | Batch_idx: 920 |  Loss: (0.0115) | Acc: (99.62%) (36700/36840)\n",
      "Epoch: 6 | Batch_idx: 930 |  Loss: (0.0115) | Acc: (99.62%) (37099/37240)\n",
      "Epoch: 6 | Batch_idx: 940 |  Loss: (0.0117) | Acc: (99.62%) (37496/37640)\n",
      "Epoch: 6 | Batch_idx: 950 |  Loss: (0.0117) | Acc: (99.62%) (37895/38040)\n",
      "Epoch: 6 | Batch_idx: 960 |  Loss: (0.0116) | Acc: (99.62%) (38295/38440)\n",
      "Epoch: 6 | Batch_idx: 970 |  Loss: (0.0117) | Acc: (99.62%) (38692/38840)\n",
      "Epoch: 6 | Batch_idx: 980 |  Loss: (0.0118) | Acc: (99.61%) (39088/39240)\n",
      "Epoch: 6 | Batch_idx: 990 |  Loss: (0.0117) | Acc: (99.61%) (39487/39640)\n",
      "Epoch: 6 | Batch_idx: 1000 |  Loss: (0.0118) | Acc: (99.62%) (39886/40040)\n",
      "Epoch: 6 | Batch_idx: 1010 |  Loss: (0.0118) | Acc: (99.61%) (40284/40440)\n",
      "Epoch: 6 | Batch_idx: 1020 |  Loss: (0.0117) | Acc: (99.62%) (40684/40840)\n",
      "Epoch: 6 | Batch_idx: 1030 |  Loss: (0.0117) | Acc: (99.62%) (41082/41240)\n",
      "Epoch: 6 | Batch_idx: 1040 |  Loss: (0.0118) | Acc: (99.62%) (41480/41640)\n",
      "Epoch: 6 | Batch_idx: 1050 |  Loss: (0.0118) | Acc: (99.62%) (41879/42040)\n",
      "Epoch: 6 | Batch_idx: 1060 |  Loss: (0.0118) | Acc: (99.62%) (42279/42440)\n",
      "Epoch: 6 | Batch_idx: 1070 |  Loss: (0.0118) | Acc: (99.62%) (42676/42840)\n",
      "Epoch: 6 | Batch_idx: 1080 |  Loss: (0.0117) | Acc: (99.62%) (43075/43240)\n",
      "Epoch: 6 | Batch_idx: 1090 |  Loss: (0.0116) | Acc: (99.62%) (43474/43640)\n",
      "Epoch: 6 | Batch_idx: 1100 |  Loss: (0.0116) | Acc: (99.62%) (43872/44040)\n",
      "Epoch: 6 | Batch_idx: 1110 |  Loss: (0.0116) | Acc: (99.62%) (44272/44440)\n",
      "Epoch: 6 | Batch_idx: 1120 |  Loss: (0.0116) | Acc: (99.62%) (44670/44840)\n",
      "Epoch: 6 | Batch_idx: 1130 |  Loss: (0.0115) | Acc: (99.62%) (45069/45240)\n",
      "Epoch: 6 | Batch_idx: 1140 |  Loss: (0.0115) | Acc: (99.62%) (45468/45640)\n",
      "Epoch: 6 | Batch_idx: 1150 |  Loss: (0.0115) | Acc: (99.62%) (45865/46040)\n",
      "Epoch: 6 | Batch_idx: 1160 |  Loss: (0.0114) | Acc: (99.62%) (46263/46440)\n",
      "Epoch: 6 | Batch_idx: 1170 |  Loss: (0.0115) | Acc: (99.62%) (46661/46840)\n",
      "Epoch: 6 | Batch_idx: 1180 |  Loss: (0.0114) | Acc: (99.62%) (47060/47240)\n",
      "Epoch: 6 | Batch_idx: 1190 |  Loss: (0.0113) | Acc: (99.62%) (47459/47640)\n",
      "Epoch: 6 | Batch_idx: 1200 |  Loss: (0.0113) | Acc: (99.62%) (47858/48040)\n",
      "Epoch: 6 | Batch_idx: 1210 |  Loss: (0.0114) | Acc: (99.62%) (48257/48440)\n",
      "Epoch: 6 | Batch_idx: 1220 |  Loss: (0.0114) | Acc: (99.62%) (48656/48840)\n",
      "Epoch: 6 | Batch_idx: 1230 |  Loss: (0.0113) | Acc: (99.63%) (49056/49240)\n",
      "Epoch: 6 | Batch_idx: 1240 |  Loss: (0.0112) | Acc: (99.63%) (49456/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1938) | Acc: (95.57%) (9557/10000)\n",
      "Epoch: 7 | Batch_idx: 0 |  Loss: (0.0252) | Acc: (97.50%) (39/40)\n",
      "Epoch: 7 | Batch_idx: 10 |  Loss: (0.0078) | Acc: (99.77%) (439/440)\n",
      "Epoch: 7 | Batch_idx: 20 |  Loss: (0.0116) | Acc: (99.76%) (838/840)\n",
      "Epoch: 7 | Batch_idx: 30 |  Loss: (0.0093) | Acc: (99.84%) (1238/1240)\n",
      "Epoch: 7 | Batch_idx: 40 |  Loss: (0.0087) | Acc: (99.76%) (1636/1640)\n",
      "Epoch: 7 | Batch_idx: 50 |  Loss: (0.0088) | Acc: (99.71%) (2034/2040)\n",
      "Epoch: 7 | Batch_idx: 60 |  Loss: (0.0085) | Acc: (99.71%) (2433/2440)\n",
      "Epoch: 7 | Batch_idx: 70 |  Loss: (0.0081) | Acc: (99.75%) (2833/2840)\n",
      "Epoch: 7 | Batch_idx: 80 |  Loss: (0.0075) | Acc: (99.78%) (3233/3240)\n",
      "Epoch: 7 | Batch_idx: 90 |  Loss: (0.0088) | Acc: (99.75%) (3631/3640)\n",
      "Epoch: 7 | Batch_idx: 100 |  Loss: (0.0084) | Acc: (99.78%) (4031/4040)\n",
      "Epoch: 7 | Batch_idx: 110 |  Loss: (0.0101) | Acc: (99.77%) (4430/4440)\n",
      "Epoch: 7 | Batch_idx: 120 |  Loss: (0.0097) | Acc: (99.77%) (4829/4840)\n",
      "Epoch: 7 | Batch_idx: 130 |  Loss: (0.0098) | Acc: (99.77%) (5228/5240)\n",
      "Epoch: 7 | Batch_idx: 140 |  Loss: (0.0100) | Acc: (99.77%) (5627/5640)\n",
      "Epoch: 7 | Batch_idx: 150 |  Loss: (0.0100) | Acc: (99.74%) (6024/6040)\n",
      "Epoch: 7 | Batch_idx: 160 |  Loss: (0.0104) | Acc: (99.70%) (6421/6440)\n",
      "Epoch: 7 | Batch_idx: 170 |  Loss: (0.0098) | Acc: (99.72%) (6821/6840)\n",
      "Epoch: 7 | Batch_idx: 180 |  Loss: (0.0104) | Acc: (99.70%) (7218/7240)\n",
      "Epoch: 7 | Batch_idx: 190 |  Loss: (0.0108) | Acc: (99.67%) (7615/7640)\n",
      "Epoch: 7 | Batch_idx: 200 |  Loss: (0.0108) | Acc: (99.66%) (8013/8040)\n",
      "Epoch: 7 | Batch_idx: 210 |  Loss: (0.0105) | Acc: (99.67%) (8412/8440)\n",
      "Epoch: 7 | Batch_idx: 220 |  Loss: (0.0102) | Acc: (99.68%) (8812/8840)\n",
      "Epoch: 7 | Batch_idx: 230 |  Loss: (0.0098) | Acc: (99.70%) (9212/9240)\n",
      "Epoch: 7 | Batch_idx: 240 |  Loss: (0.0099) | Acc: (99.70%) (9611/9640)\n",
      "Epoch: 7 | Batch_idx: 250 |  Loss: (0.0098) | Acc: (99.70%) (10010/10040)\n",
      "Epoch: 7 | Batch_idx: 260 |  Loss: (0.0099) | Acc: (99.70%) (10409/10440)\n",
      "Epoch: 7 | Batch_idx: 270 |  Loss: (0.0103) | Acc: (99.70%) (10807/10840)\n",
      "Epoch: 7 | Batch_idx: 280 |  Loss: (0.0103) | Acc: (99.70%) (11206/11240)\n",
      "Epoch: 7 | Batch_idx: 290 |  Loss: (0.0102) | Acc: (99.69%) (11604/11640)\n",
      "Epoch: 7 | Batch_idx: 300 |  Loss: (0.0104) | Acc: (99.68%) (12002/12040)\n",
      "Epoch: 7 | Batch_idx: 310 |  Loss: (0.0103) | Acc: (99.69%) (12402/12440)\n",
      "Epoch: 7 | Batch_idx: 320 |  Loss: (0.0102) | Acc: (99.69%) (12800/12840)\n",
      "Epoch: 7 | Batch_idx: 330 |  Loss: (0.0100) | Acc: (99.70%) (13200/13240)\n",
      "Epoch: 7 | Batch_idx: 340 |  Loss: (0.0098) | Acc: (99.71%) (13600/13640)\n",
      "Epoch: 7 | Batch_idx: 350 |  Loss: (0.0100) | Acc: (99.71%) (13999/14040)\n",
      "Epoch: 7 | Batch_idx: 360 |  Loss: (0.0098) | Acc: (99.72%) (14399/14440)\n",
      "Epoch: 7 | Batch_idx: 370 |  Loss: (0.0100) | Acc: (99.71%) (14797/14840)\n",
      "Epoch: 7 | Batch_idx: 380 |  Loss: (0.0098) | Acc: (99.72%) (15197/15240)\n",
      "Epoch: 7 | Batch_idx: 390 |  Loss: (0.0097) | Acc: (99.73%) (15597/15640)\n",
      "Epoch: 7 | Batch_idx: 400 |  Loss: (0.0095) | Acc: (99.73%) (15997/16040)\n",
      "Epoch: 7 | Batch_idx: 410 |  Loss: (0.0094) | Acc: (99.73%) (16396/16440)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Batch_idx: 420 |  Loss: (0.0096) | Acc: (99.72%) (16793/16840)\n",
      "Epoch: 7 | Batch_idx: 430 |  Loss: (0.0094) | Acc: (99.73%) (17193/17240)\n",
      "Epoch: 7 | Batch_idx: 440 |  Loss: (0.0096) | Acc: (99.73%) (17592/17640)\n",
      "Epoch: 7 | Batch_idx: 450 |  Loss: (0.0096) | Acc: (99.73%) (17991/18040)\n",
      "Epoch: 7 | Batch_idx: 460 |  Loss: (0.0099) | Acc: (99.72%) (18388/18440)\n",
      "Epoch: 7 | Batch_idx: 470 |  Loss: (0.0099) | Acc: (99.72%) (18787/18840)\n",
      "Epoch: 7 | Batch_idx: 480 |  Loss: (0.0099) | Acc: (99.71%) (19185/19240)\n",
      "Epoch: 7 | Batch_idx: 490 |  Loss: (0.0099) | Acc: (99.71%) (19583/19640)\n",
      "Epoch: 7 | Batch_idx: 500 |  Loss: (0.0098) | Acc: (99.72%) (19983/20040)\n",
      "Epoch: 7 | Batch_idx: 510 |  Loss: (0.0097) | Acc: (99.72%) (20382/20440)\n",
      "Epoch: 7 | Batch_idx: 520 |  Loss: (0.0098) | Acc: (99.71%) (20779/20840)\n",
      "Epoch: 7 | Batch_idx: 530 |  Loss: (0.0097) | Acc: (99.71%) (21179/21240)\n",
      "Epoch: 7 | Batch_idx: 540 |  Loss: (0.0097) | Acc: (99.71%) (21578/21640)\n",
      "Epoch: 7 | Batch_idx: 550 |  Loss: (0.0097) | Acc: (99.71%) (21977/22040)\n",
      "Epoch: 7 | Batch_idx: 560 |  Loss: (0.0096) | Acc: (99.71%) (22375/22440)\n",
      "Epoch: 7 | Batch_idx: 570 |  Loss: (0.0095) | Acc: (99.72%) (22775/22840)\n",
      "Epoch: 7 | Batch_idx: 580 |  Loss: (0.0095) | Acc: (99.72%) (23174/23240)\n",
      "Epoch: 7 | Batch_idx: 590 |  Loss: (0.0095) | Acc: (99.71%) (23572/23640)\n",
      "Epoch: 7 | Batch_idx: 600 |  Loss: (0.0094) | Acc: (99.71%) (23971/24040)\n",
      "Epoch: 7 | Batch_idx: 610 |  Loss: (0.0094) | Acc: (99.71%) (24370/24440)\n",
      "Epoch: 7 | Batch_idx: 620 |  Loss: (0.0093) | Acc: (99.72%) (24770/24840)\n",
      "Epoch: 7 | Batch_idx: 630 |  Loss: (0.0092) | Acc: (99.72%) (25169/25240)\n",
      "Epoch: 7 | Batch_idx: 640 |  Loss: (0.0092) | Acc: (99.72%) (25568/25640)\n",
      "Epoch: 7 | Batch_idx: 650 |  Loss: (0.0091) | Acc: (99.72%) (25968/26040)\n",
      "Epoch: 7 | Batch_idx: 660 |  Loss: (0.0090) | Acc: (99.73%) (26368/26440)\n",
      "Epoch: 7 | Batch_idx: 670 |  Loss: (0.0091) | Acc: (99.72%) (26766/26840)\n",
      "Epoch: 7 | Batch_idx: 680 |  Loss: (0.0091) | Acc: (99.72%) (27164/27240)\n",
      "Epoch: 7 | Batch_idx: 690 |  Loss: (0.0091) | Acc: (99.73%) (27564/27640)\n",
      "Epoch: 7 | Batch_idx: 700 |  Loss: (0.0090) | Acc: (99.73%) (27964/28040)\n",
      "Epoch: 7 | Batch_idx: 710 |  Loss: (0.0089) | Acc: (99.73%) (28364/28440)\n",
      "Epoch: 7 | Batch_idx: 720 |  Loss: (0.0089) | Acc: (99.73%) (28763/28840)\n",
      "Epoch: 7 | Batch_idx: 730 |  Loss: (0.0088) | Acc: (99.74%) (29163/29240)\n",
      "Epoch: 7 | Batch_idx: 740 |  Loss: (0.0088) | Acc: (99.74%) (29562/29640)\n",
      "Epoch: 7 | Batch_idx: 750 |  Loss: (0.0087) | Acc: (99.74%) (29962/30040)\n",
      "Epoch: 7 | Batch_idx: 760 |  Loss: (0.0087) | Acc: (99.74%) (30361/30440)\n",
      "Epoch: 7 | Batch_idx: 770 |  Loss: (0.0086) | Acc: (99.74%) (30761/30840)\n",
      "Epoch: 7 | Batch_idx: 780 |  Loss: (0.0086) | Acc: (99.75%) (31161/31240)\n",
      "Epoch: 7 | Batch_idx: 790 |  Loss: (0.0086) | Acc: (99.74%) (31559/31640)\n",
      "Epoch: 7 | Batch_idx: 800 |  Loss: (0.0086) | Acc: (99.74%) (31958/32040)\n",
      "Epoch: 7 | Batch_idx: 810 |  Loss: (0.0086) | Acc: (99.74%) (32357/32440)\n",
      "Epoch: 7 | Batch_idx: 820 |  Loss: (0.0086) | Acc: (99.74%) (32756/32840)\n",
      "Epoch: 7 | Batch_idx: 830 |  Loss: (0.0087) | Acc: (99.74%) (33155/33240)\n",
      "Epoch: 7 | Batch_idx: 840 |  Loss: (0.0089) | Acc: (99.74%) (33552/33640)\n",
      "Epoch: 7 | Batch_idx: 850 |  Loss: (0.0089) | Acc: (99.74%) (33952/34040)\n",
      "Epoch: 7 | Batch_idx: 860 |  Loss: (0.0093) | Acc: (99.73%) (34348/34440)\n",
      "Epoch: 7 | Batch_idx: 870 |  Loss: (0.0092) | Acc: (99.74%) (34748/34840)\n",
      "Epoch: 7 | Batch_idx: 880 |  Loss: (0.0093) | Acc: (99.74%) (35147/35240)\n",
      "Epoch: 7 | Batch_idx: 890 |  Loss: (0.0093) | Acc: (99.74%) (35546/35640)\n",
      "Epoch: 7 | Batch_idx: 900 |  Loss: (0.0094) | Acc: (99.73%) (35943/36040)\n",
      "Epoch: 7 | Batch_idx: 910 |  Loss: (0.0094) | Acc: (99.73%) (36341/36440)\n",
      "Epoch: 7 | Batch_idx: 920 |  Loss: (0.0096) | Acc: (99.72%) (36738/36840)\n",
      "Epoch: 7 | Batch_idx: 930 |  Loss: (0.0096) | Acc: (99.72%) (37137/37240)\n",
      "Epoch: 7 | Batch_idx: 940 |  Loss: (0.0096) | Acc: (99.72%) (37536/37640)\n",
      "Epoch: 7 | Batch_idx: 950 |  Loss: (0.0095) | Acc: (99.72%) (37935/38040)\n",
      "Epoch: 7 | Batch_idx: 960 |  Loss: (0.0094) | Acc: (99.73%) (38335/38440)\n",
      "Epoch: 7 | Batch_idx: 970 |  Loss: (0.0094) | Acc: (99.73%) (38734/38840)\n",
      "Epoch: 7 | Batch_idx: 980 |  Loss: (0.0094) | Acc: (99.73%) (39133/39240)\n",
      "Epoch: 7 | Batch_idx: 990 |  Loss: (0.0093) | Acc: (99.73%) (39533/39640)\n",
      "Epoch: 7 | Batch_idx: 1000 |  Loss: (0.0093) | Acc: (99.73%) (39932/40040)\n",
      "Epoch: 7 | Batch_idx: 1010 |  Loss: (0.0094) | Acc: (99.72%) (40327/40440)\n",
      "Epoch: 7 | Batch_idx: 1020 |  Loss: (0.0094) | Acc: (99.72%) (40727/40840)\n",
      "Epoch: 7 | Batch_idx: 1030 |  Loss: (0.0094) | Acc: (99.72%) (41126/41240)\n",
      "Epoch: 7 | Batch_idx: 1040 |  Loss: (0.0094) | Acc: (99.72%) (41525/41640)\n",
      "Epoch: 7 | Batch_idx: 1050 |  Loss: (0.0093) | Acc: (99.73%) (41925/42040)\n",
      "Epoch: 7 | Batch_idx: 1060 |  Loss: (0.0093) | Acc: (99.72%) (42323/42440)\n",
      "Epoch: 7 | Batch_idx: 1070 |  Loss: (0.0093) | Acc: (99.72%) (42721/42840)\n",
      "Epoch: 7 | Batch_idx: 1080 |  Loss: (0.0095) | Acc: (99.72%) (43119/43240)\n",
      "Epoch: 7 | Batch_idx: 1090 |  Loss: (0.0095) | Acc: (99.72%) (43518/43640)\n",
      "Epoch: 7 | Batch_idx: 1100 |  Loss: (0.0094) | Acc: (99.72%) (43918/44040)\n",
      "Epoch: 7 | Batch_idx: 1110 |  Loss: (0.0094) | Acc: (99.72%) (44317/44440)\n",
      "Epoch: 7 | Batch_idx: 1120 |  Loss: (0.0093) | Acc: (99.73%) (44717/44840)\n",
      "Epoch: 7 | Batch_idx: 1130 |  Loss: (0.0093) | Acc: (99.73%) (45117/45240)\n",
      "Epoch: 7 | Batch_idx: 1140 |  Loss: (0.0092) | Acc: (99.73%) (45517/45640)\n",
      "Epoch: 7 | Batch_idx: 1150 |  Loss: (0.0092) | Acc: (99.73%) (45917/46040)\n",
      "Epoch: 7 | Batch_idx: 1160 |  Loss: (0.0091) | Acc: (99.74%) (46317/46440)\n",
      "Epoch: 7 | Batch_idx: 1170 |  Loss: (0.0091) | Acc: (99.74%) (46717/46840)\n",
      "Epoch: 7 | Batch_idx: 1180 |  Loss: (0.0090) | Acc: (99.74%) (47117/47240)\n",
      "Epoch: 7 | Batch_idx: 1190 |  Loss: (0.0090) | Acc: (99.74%) (47517/47640)\n",
      "Epoch: 7 | Batch_idx: 1200 |  Loss: (0.0091) | Acc: (99.74%) (47915/48040)\n",
      "Epoch: 7 | Batch_idx: 1210 |  Loss: (0.0093) | Acc: (99.74%) (48312/48440)\n",
      "Epoch: 7 | Batch_idx: 1220 |  Loss: (0.0092) | Acc: (99.74%) (48712/48840)\n",
      "Epoch: 7 | Batch_idx: 1230 |  Loss: (0.0093) | Acc: (99.74%) (49111/49240)\n",
      "Epoch: 7 | Batch_idx: 1240 |  Loss: (0.0094) | Acc: (99.74%) (49510/49640)\n",
      "=> saving checkpoint\n",
      "# TEST : Loss: (0.1896) | Acc: (95.54%) (9554/10000)\n",
      "Epoch: 8 | Batch_idx: 0 |  Loss: (0.0007) | Acc: (100.00%) (40/40)\n",
      "Epoch: 8 | Batch_idx: 10 |  Loss: (0.0055) | Acc: (100.00%) (440/440)\n",
      "Epoch: 8 | Batch_idx: 20 |  Loss: (0.0049) | Acc: (99.88%) (839/840)\n"
     ]
    }
   ],
   "source": [
    "# Main function for training the model\n",
    "\n",
    "TRAIN_LOSS = []\n",
    "TEST_LOSS = []\n",
    "TRAIN_ACC = []\n",
    "TEST_ACC = []\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "checkpoint = load_checkpoint(default_directory)\n",
    "if not checkpoint:\n",
    "    pass\n",
    "else:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "writer = SummaryWriter(root_dir)    \n",
    "\n",
    "for epoch in range(start_epoch, num_of_epochs):\n",
    "\n",
    "    if epoch < 20:\n",
    "        lr = learning_rate\n",
    "    elif epoch < 40:\n",
    "        lr = learning_rate * 0.1\n",
    "    else:\n",
    "        lr = learning_rate * 0.01\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    train(epoch)\n",
    "    save_checkpoint(default_directory, {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    })\n",
    "    test()  \n",
    "\n",
    "writer.close()\n",
    "    \n",
    "now = time.gmtime(time.time() - start_time)\n",
    "print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ea237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
